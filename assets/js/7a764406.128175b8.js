"use strict";(self.webpackChunkwyj_lab=self.webpackChunkwyj_lab||[]).push([[2591],{27869:a=>{a.exports=JSON.parse('{"label":"CLIP","permalink":"/docs/tags/clip","allTagsPath":"/docs/tags","count":3,"items":[{"id":"Paper/Vision-Language/Contrastive Learning/2021-03-CLIP","title":"Learning Transferable Visual Models From Natural Language Supervision","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Contrastive Learning/CLIP"},{"id":"Paper/Vision-Language/PEFT/Composition/2024-05-CLIP-LoRA","title":"Low-Rank Few-Shot Adaptation of Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Composition/CLIP-LoRA"},{"id":"Paper/Vision-Language/PEFT/Multi-Modality/2022-10-MaPLe","title":"MaPLe: Multi-modal Prompt Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Multi-Modality/MaPLe"}]}')}}]);