"use strict";(self.webpackChunkwyj_lab=self.webpackChunkwyj_lab||[]).push([[3258],{258:i=>{i.exports=JSON.parse('{"label":"PEFT","permalink":"/docs/tags/peft","allTagsPath":"/docs/tags","count":14,"items":[{"id":"Paper/NLP/PEFT/2023-09-20-AdaLoRA","title":"Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/AdaLoRA"},{"id":"Paper/NLP/PEFT/2023-10-29-ATTEMPT","title":"ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/ATTEMPT"},{"id":"Paper/NLP/PEFT/2022-11-13-DEPT","title":"DEPT: Decomposed Prompt Tuning For Parameter-Efficient Fine Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/DEPT"},{"id":"Paper/NLP/PEFT/2023-09-17-IA\xb3","title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/IA\xb3"},{"id":"Paper/NLP/PEFT/2023-09-11-P-tuning","title":"GPT Understands, Too","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/P-tuning"},{"id":"Paper/NLP/PEFT/2023-09-09-LLaMA-Adapter V2","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/LLaMA-Adapter V2"},{"id":"Paper/NLP/PEFT/2023-09-08-LLaMA-Adapter","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/LLaMA-Adapter"},{"id":"Paper/NLP/PEFT/2023-09-06-LoRA","title":"LoRA: Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/LoRA"},{"id":"Paper/NLP/PEFT/2023-08-31-MPT","title":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Multitask Prompt Tuning"},{"id":"Paper/NLP/PEFT/2023-09-13-P-tuning v2","title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/P-tuning v2"},{"id":"Paper/NLP/PEFT/2023-09-14-Prefix-Tuning","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Prefix-Tuning"},{"id":"Paper/NLP/Prompt Tuning/2023-11-13-PTR","title":"PTR: Prompt Tuning with Rules for Text Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","permalink":"/docs/Paper/NLP/Prompt Tuning/PTR"},{"id":"Paper/NLP/PEFT/2023-10-27-Residual-Prompt-Tuning","title":"Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","permalink":"/docs/Paper/NLP/PEFT/Residual Prompt Tuning"},{"id":"Paper/NLP/PEFT/2023-09-16-Prompt-Tuning","title":"The Power of Scale for Parameter-Efficient Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Prompt Tuning"}]}')}}]);