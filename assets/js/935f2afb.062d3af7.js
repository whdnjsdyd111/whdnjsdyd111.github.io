"use strict";(self.webpackChunkwyj_lab=self.webpackChunkwyj_lab||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Tutorial Intro","href":"/docs/intro","docId":"intro"},{"type":"category","label":"Tutorial - Basics","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Create a Page","href":"/docs/tutorial-basics/create-a-page","docId":"tutorial-basics/create-a-page"},{"type":"link","label":"Create a Document","href":"/docs/tutorial-basics/create-a-document","docId":"tutorial-basics/create-a-document"},{"type":"link","label":"Create a Blog Post","href":"/docs/tutorial-basics/create-a-blog-post","docId":"tutorial-basics/create-a-blog-post"},{"type":"link","label":"Markdown Features","href":"/docs/tutorial-basics/markdown-features","docId":"tutorial-basics/markdown-features"},{"type":"link","label":"Deploy your site","href":"/docs/tutorial-basics/deploy-your-site","docId":"tutorial-basics/deploy-your-site"},{"type":"link","label":"Congratulations!","href":"/docs/tutorial-basics/congratulations","docId":"tutorial-basics/congratulations"}],"href":"/docs/category/tutorial---basics"},{"type":"category","label":"Tutorial - Extras","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Manage Docs Versions","href":"/docs/tutorial-extras/manage-docs-versions","docId":"tutorial-extras/manage-docs-versions"},{"type":"link","label":"Translate your site","href":"/docs/tutorial-extras/translate-your-site","docId":"tutorial-extras/translate-your-site"}],"href":"/docs/category/tutorial---extras"},{"type":"category","label":"Paper","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Computer Vision","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Adversarial Attack","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Adversarial Reprogramming of Neural Networks","href":"/docs/Paper/Computer Vision/Adversarial Attack/AR","docId":"Paper/Computer Vision/Adversarial Attack/2018-06-AR"}]},{"type":"category","label":"Image Classification","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"An Image Is Worth 16X16 Words: Transformers for image recognition at sacle","href":"/docs/Paper/Computer Vision/Image Classification/ViT","docId":"Paper/Computer Vision/Image Classification/2020-10-ViT"},{"type":"link","label":"EfficientNetV2: Smaller Models and Faster Training","href":"/docs/Paper/Computer Vision/Image Classification/EfficientNetV2","docId":"Paper/Computer Vision/Image Classification/2021-04-EfficientNetV2"}]},{"type":"category","label":"Multi-task","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"A Unified Sequence Interface for Vision Tasks","href":"/docs/Paper/Computer Vision/Multi-task/Unified Interface","docId":"Paper/Computer Vision/Multi-task/2022-06-Unified Interface"},{"type":"link","label":"UNINEXT: Universal Instance Perception as Object Discovery and Retrieval","href":"/docs/Paper/Computer Vision/Multi-task/UNINEXT","docId":"Paper/Computer Vision/Multi-task/2023-03-UNINEXT"}]}]},{"type":"category","label":"NLP","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Analysis","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings","href":"/docs/Paper/NLP/Analysis/Contextualized Representation","docId":"Paper/NLP/Analysis/2019-11-Context_Representation"}]},{"type":"category","label":"Augmentation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"PromptDA : Label-guided Data Augmentation for Prompt-based Few Shot Learners","href":"/docs/Paper/NLP/Augmentation/PromptDA","docId":"Paper/NLP/Augmentation/2023-05-PromptDA"}]},{"type":"category","label":"Model","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Attention Is All You Need","href":"/docs/Paper/NLP/Model/Transformer","docId":"Paper/NLP/Model/2017-06-Transformer"},{"type":"link","label":"Improving Language Understanding by Generative Pre-Training","href":"/docs/Paper/NLP/Model/GPT-1","docId":"Paper/NLP/Model/2018-06-GPT"},{"type":"link","label":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","href":"/docs/Paper/NLP/Model/BERT","docId":"Paper/NLP/Model/2018-10-BERT"}]},{"type":"category","label":"Multi-Task","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","href":"/docs/Paper/NLP/Multi-Task/Chain-of-Thought","docId":"Paper/NLP/Multi-Task/2022-01-CoT"},{"type":"link","label":"Scaling Instruction-Finetuned Language Models","href":"/docs/Paper/NLP/Multi-Task/Flan-T5","docId":"Paper/NLP/Multi-Task/2022-10-Flan-T5"},{"type":"link","label":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation","href":"/docs/Paper/NLP/Multi-Task/CodeT5+","docId":"Paper/NLP/Multi-Task/2023-05-CodeT5p"}]},{"type":"category","label":"PEFT","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Composition","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"LoRA: Low-Rank Adaptation of Large Language Models","href":"/docs/Paper/NLP/PEFT/Composition/LoRA","docId":"Paper/NLP/PEFT/Composition/2021-06-LoRA"},{"type":"link","label":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models","href":"/docs/Paper/NLP/PEFT/Composition/BitFit","docId":"Paper/NLP/PEFT/Composition/2022-05-BitFit"},{"type":"link","label":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning","href":"/docs/Paper/NLP/PEFT/Composition/IA\xb3","docId":"Paper/NLP/PEFT/Composition/2022-05-IA\xb3"},{"type":"link","label":"Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning","href":"/docs/Paper/NLP/PEFT/Composition/AdaLoRA","docId":"Paper/NLP/PEFT/Composition/2023-03-AdaLoRA"}]},{"type":"category","label":"Mixture","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning","href":"/docs/Paper/NLP/PEFT/Mixture/UniPELT","docId":"Paper/NLP/PEFT/Mixture/2022-05-UniPELT"},{"type":"link","label":"Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients","href":"/docs/Paper/NLP/PEFT/Mixture/PEFT without Its Gradients","docId":"Paper/NLP/PEFT/Mixture/2023-12-No-Gradients"}]},{"type":"category","label":"Module","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Parameter-Efficient Transfer Learning for NLP","href":"/docs/Paper/NLP/PEFT/Module/Adapter","docId":"Paper/NLP/PEFT/Module/2019-02-Adapter"}]},{"type":"category","label":"Pruning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Pruning Pre-trained Language Models Without Fine-Tuning","href":"/docs/Paper/NLP/PEFT/Pruning/SMP","docId":"Paper/NLP/PEFT/Pruning/2023-07-SMP"}]},{"type":"category","label":"Soft Prompt","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Prefix-Tuning: Optimizing Continuous Prompts for Generation","href":"/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning","docId":"Paper/NLP/PEFT/Soft Prompt/2021-01-Prefix-Tuning"},{"type":"link","label":"GPT Understands, Too","href":"/docs/Paper/NLP/PEFT/Soft Prompt/P-tuning","docId":"Paper/NLP/PEFT/Soft Prompt/2021-03-P-tuning"},{"type":"link","label":"The Power of Scale for Parameter-Efficient Prompt Tuning","href":"/docs/Paper/NLP/PEFT/Soft Prompt/Prompt Tuning","docId":"Paper/NLP/PEFT/Soft Prompt/2021-04-Prompt-Tuning"},{"type":"link","label":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks","href":"/docs/Paper/NLP/PEFT/Soft Prompt/P-tuning v2","docId":"Paper/NLP/PEFT/Soft Prompt/2021-10-P-tuning v2"},{"type":"link","label":"ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts","href":"/docs/Paper/NLP/PEFT/Soft Prompt/ATTEMPT","docId":"Paper/NLP/PEFT/Soft Prompt/2022-05-ATTEMPT"},{"type":"link","label":"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer","href":"/docs/Paper/NLP/PEFT/Soft Prompt/SPoT","docId":"Paper/NLP/PEFT/Soft Prompt/2022-05-SPoT"},{"type":"link","label":"XPrompt: Exploring the Extreme of Prompt Tuning","href":"/docs/Paper/NLP/PEFT/Soft Prompt/XPrompt","docId":"Paper/NLP/PEFT/Soft Prompt/2022-12-XPrompt"},{"type":"link","label":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","href":"/docs/Paper/NLP/PEFT/Soft Prompt/LLaMA-Adapter","docId":"Paper/NLP/PEFT/Soft Prompt/2023-03-LLaMA-Adapter"},{"type":"link","label":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning","href":"/docs/Paper/NLP/PEFT/Soft Prompt/Multitask Prompt Tuning","docId":"Paper/NLP/PEFT/Soft Prompt/2023-03-MPT"},{"type":"link","label":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","href":"/docs/Paper/NLP/PEFT/Soft Prompt/LLaMA-Adapter V2","docId":"Paper/NLP/PEFT/Soft Prompt/2023-04-LLaMA-Adapter V2"},{"type":"link","label":"Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization","href":"/docs/Paper/NLP/PEFT/Soft Prompt/Residual Prompt Tuning","docId":"Paper/NLP/PEFT/Soft Prompt/2023-05-Residual-Prompt-Tuning"},{"type":"link","label":"DEPT: Decomposed Prompt Tuning For Parameter-Efficient Fine Tuning","href":"/docs/Paper/NLP/PEFT/Soft Prompt/DEPT","docId":"Paper/NLP/PEFT/Soft Prompt/2023-09-DEPT"},{"type":"link","label":"APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models","href":"/docs/Paper/NLP/PEFT/Soft Prompt/APrompt","docId":"Paper/NLP/PEFT/Soft Prompt/2023-12-APrompt"},{"type":"link","label":"SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts","href":"/docs/Paper/NLP/PEFT/Soft Prompt/SMoP","docId":"Paper/NLP/PEFT/Soft Prompt/2023-12-SMoP"}]}]},{"type":"category","label":"Prompt Tuning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"PTR: Prompt Tuning with Rules for Text Classification","href":"/docs/Paper/NLP/Prompt Tuning/PTR","docId":"Paper/NLP/Prompt Tuning/2022-11-PTR"}]},{"type":"category","label":"Reinforcement Learning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Reflexion: Language Agents with Verbal Reinforcement Learning","href":"/docs/Paper/NLP/Reinforcement Learning/Reflexion","docId":"Paper/NLP/Reinforcement Learning/2023-03-Reflexion"}]},{"type":"category","label":"Text Generation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Training language models to follow instructions with human feedback (+ ChatGPT)","href":"/docs/Paper/NLP/Text Generation/InstructGPT","docId":"Paper/NLP/Text Generation/2022-03-InstructGPT"}]}]},{"type":"category","label":"Survey","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","href":"/docs/Paper/Survey/Prompting","docId":"Paper/Survey/2021-07-Prompting"},{"type":"link","label":"Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey","href":"/docs/Paper/Survey/PEFT for PVMs","docId":"Paper/Survey/2024-02-PEFT Vision"}]},{"type":"category","label":"Vision-Language","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Unsupervised Prompt Learning for Vision-Language Models","href":"/docs/Paper/Vision-Language/UPL","docId":"Paper/Vision-Language/2022-04-UPL"},{"type":"link","label":"From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models","href":"/docs/Paper/Vision-Language/Img2LLM","docId":"Paper/Vision-Language/2022-12-Img2LLM"},{"type":"link","label":"Prismer: A Vision-Language Model with An Esemble of Experts","href":"/docs/Paper/Vision-Language/Prismer","docId":"Paper/Vision-Language/2023-03-Prismer"},{"type":"category","label":"Contrastive Learning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision","href":"/docs/Paper/Vision-Language/Contrastive Learning/ALIGN","docId":"Paper/Vision-Language/Contrastive Learning/2021-02-ALIGN"},{"type":"link","label":"Learning Transferable Visual Models From Natural Language Supervision","href":"/docs/Paper/Vision-Language/Contrastive Learning/CLIP","docId":"Paper/Vision-Language/Contrastive Learning/2021-03-CLIP"}]},{"type":"category","label":"PEFT","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Multi-Modality","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Adaptive Multi-Modality Prompt Learning","href":"/docs/Paper/Vision-Language/PEFT/Multi-Modality/AMMPL","docId":"Paper/Vision-Language/PEFT/Multi-Modality/2023-12-AMMPL"}]},{"type":"category","label":"Prompting","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Pixel-Level","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Cross-modal Adversarial Reprogramming","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/CMAR","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2021-02-CMAR"},{"type":"link","label":"Exploring Visual Prompts for Adapting Large-Scale Models","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/VP","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-03-VP"},{"type":"link","label":"Watermarking for Out-of-distribution Detection","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/Watermarking","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-10-Watermarking"},{"type":"link","label":"Understanding and Improving Visual Prompting: A Label-Mapping Perspective","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/ILM-VP","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-11-ILM-VP"},{"type":"link","label":"Unleashing the Power of Visual Prompting At the Pixel Level","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/EVP","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-12-EVP"},{"type":"link","label":"Diversity-Aware Meta Visual Prompting","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/DAM-VP","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-03-DAM-VP"},{"type":"link","label":"Explicit Visual Prompting for Low-Level Structure Segmentations","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/EVP-L","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-03-EVP-L"},{"type":"link","label":"Hint-Aug: Drawing Hints from Foundation Vision Transformers towards Boosted Few-shot Parameter-Efficient Tuning","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/HinTAug","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-04-HintAug"},{"type":"link","label":"AutoVP: An Automated Visual Prompting Framework and Benchmark","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/AutoVP","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-10-AutoVP"},{"type":"link","label":"A2XP: Towards Private Domain Generalization","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/A2XP","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-11-A2XP"},{"type":"link","label":"Instruct Me More! Random Prompting for Visual In-Context Learning","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/InMeMo","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-11-InMeMo"},{"type":"link","label":"LaViP: Language-Grounded Visual Prompts","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/LaViP","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-12-LaViP"},{"type":"link","label":"AdaViPro: Region-Based Adaptive Visual Prompt For Large-Scale Models Adapting","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/AdaViPro","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-03-ADAVIPRO"},{"type":"link","label":"Exploring the Transferability of Visual Prompting for Multimodal Large Language Models","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/TVP","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-04-TVP"},{"type":"link","label":"Sample-specific Masks for Visual Reprogramming-based Prompting","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/SMM","docId":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-06-SMM"}]},{"type":"category","label":"Visual-Token","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Visual Prompt Tuning","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/VPT","docId":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-03-VPT"},{"type":"link","label":"Visual Prompt Tuning For Test-time Domain Adaptation","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/DePT","docId":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-10-DePT"},{"type":"link","label":"LPT: Long-Tailed Prompt Tuning For Image Classification","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/LPT","docId":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-10-LPT"},{"type":"link","label":"Convolutional Visual Prompt for Robust Visual Perception","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/CVP","docId":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-03-CVP"},{"type":"link","label":"E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/E2VPT","docId":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-07-E2VPT"},{"type":"link","label":"SA^2VPT: Spatially Aligned-and-Adapted Visual Prompt","href":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/SA2VP","docId":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-12-SA2VP"}]}]}]},{"type":"category","label":"Single-Stream","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"VL-BERT: Pre-Training Of Generic Visual-Linguistic Representations","href":"/docs/Paper/Vision-Language/Single-Stream/VLBERT","docId":"Paper/Vision-Language/Single-Stream/2019-08-VLBERT"},{"type":"link","label":"VisualBERT: A Simple And Performance Baseline For Visual And Language","href":"/docs/Paper/Vision-Language/Single-Stream/VisualBERT","docId":"Paper/Vision-Language/Single-Stream/2019-08-VisualBERT"},{"type":"link","label":"UNITER: UNiversal Image-TExt Representation Learning","href":"/docs/Paper/Vision-Language/Single-Stream/UNITER","docId":"Paper/Vision-Language/Single-Stream/2019-09-UNITER"},{"type":"link","label":"ImageBERT: Cross-Modal Pre-Training with Large-Scale Weak-Supervised Image-Text Data","href":"/docs/Paper/Vision-Language/Single-Stream/ImageBERT","docId":"Paper/Vision-Language/Single-Stream/2020-01-ImageBERT"},{"type":"link","label":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers","href":"/docs/Paper/Vision-Language/Single-Stream/Pixel-BERT","docId":"Paper/Vision-Language/Single-Stream/2020-04-Pixel-BERT"},{"type":"link","label":"VD-BERT: A Unified Vision and Dialog Transformer with BERT","href":"/docs/Paper/Vision-Language/Single-Stream/VD-BERT","docId":"Paper/Vision-Language/Single-Stream/2020-04-VD-BERT"}]},{"type":"category","label":"Two-Stream","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers","href":"/docs/Paper/Vision-Language/Two-Stream/LXMERT","docId":"Paper/Vision-Language/Two-Stream/2019-08-LXMERT"},{"type":"link","label":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks","href":"/docs/Paper/Vision-Language/Two-Stream/ViLBERT","docId":"Paper/Vision-Language/Two-Stream/2019-08-ViLBERT"}]}]}]},{"type":"category","label":"Programming","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"What is the Algorithm?","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"BackJoon","href":"/docs/Programming/Algorithm/Problems","docId":"Programming/Algorithm/Problems"}],"href":"/docs/Programming/Algorithm/"}]}]},"docs":{"intro":{"id":"intro","title":"Tutorial Intro","description":"Let\'s discover Docusaurus in less than 5 minutes.","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Adversarial Attack/2018-06-AR":{"id":"Paper/Computer Vision/Adversarial Attack/2018-06-AR","title":"Adversarial Reprogramming of Neural Networks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Image Classification/2020-10-ViT":{"id":"Paper/Computer Vision/Image Classification/2020-10-ViT","title":"An Image Is Worth 16X16 Words: Transformers for image recognition at sacle","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Image Classification/2021-04-EfficientNetV2":{"id":"Paper/Computer Vision/Image Classification/2021-04-EfficientNetV2","title":"EfficientNetV2: Smaller Models and Faster Training","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Multi-task/2022-06-Unified Interface":{"id":"Paper/Computer Vision/Multi-task/2022-06-Unified Interface","title":"A Unified Sequence Interface for Vision Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Multi-task/2023-03-UNINEXT":{"id":"Paper/Computer Vision/Multi-task/2023-03-UNINEXT","title":"UNINEXT: Universal Instance Perception as Object Discovery and Retrieval","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Analysis/2019-11-Context_Representation":{"id":"Paper/NLP/Analysis/2019-11-Context_Representation","title":"How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Augmentation/2023-05-PromptDA":{"id":"Paper/NLP/Augmentation/2023-05-PromptDA","title":"PromptDA : Label-guided Data Augmentation for Prompt-based Few Shot Learners","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Model/2017-06-Transformer":{"id":"Paper/NLP/Model/2017-06-Transformer","title":"Attention Is All You Need","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Model/2018-06-GPT":{"id":"Paper/NLP/Model/2018-06-GPT","title":"Improving Language Understanding by Generative Pre-Training","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Model/2018-10-BERT":{"id":"Paper/NLP/Model/2018-10-BERT","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","description":"\ub17c\ubb38 \uc774\ubbf8\uc9c0 \ubc0f \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Multi-Task/2022-01-CoT":{"id":"Paper/NLP/Multi-Task/2022-01-CoT","title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Multi-Task/2022-10-Flan-T5":{"id":"Paper/NLP/Multi-Task/2022-10-Flan-T5","title":"Scaling Instruction-Finetuned Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Multi-Task/2023-05-CodeT5p":{"id":"Paper/NLP/Multi-Task/2023-05-CodeT5p","title":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2021-06-LoRA":{"id":"Paper/NLP/PEFT/Composition/2021-06-LoRA","title":"LoRA: Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2022-05-BitFit":{"id":"Paper/NLP/PEFT/Composition/2022-05-BitFit","title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2022-05-IA\xb3":{"id":"Paper/NLP/PEFT/Composition/2022-05-IA\xb3","title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2023-03-AdaLoRA":{"id":"Paper/NLP/PEFT/Composition/2023-03-AdaLoRA","title":"Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Mixture/2022-05-UniPELT":{"id":"Paper/NLP/PEFT/Mixture/2022-05-UniPELT","title":"UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Mixture/2023-12-No-Gradients":{"id":"Paper/NLP/PEFT/Mixture/2023-12-No-Gradients","title":"Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Module/2019-02-Adapter":{"id":"Paper/NLP/PEFT/Module/2019-02-Adapter","title":"Parameter-Efficient Transfer Learning for NLP","description":"\ub17c\ubb38 \uc774\ubbf8\uc9c0 \ubc0f \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Pruning/2023-07-SMP":{"id":"Paper/NLP/PEFT/Pruning/2023-07-SMP","title":"Pruning Pre-trained Language Models Without Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2021-01-Prefix-Tuning":{"id":"Paper/NLP/PEFT/Soft Prompt/2021-01-Prefix-Tuning","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2021-03-P-tuning":{"id":"Paper/NLP/PEFT/Soft Prompt/2021-03-P-tuning","title":"GPT Understands, Too","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2021-04-Prompt-Tuning":{"id":"Paper/NLP/PEFT/Soft Prompt/2021-04-Prompt-Tuning","title":"The Power of Scale for Parameter-Efficient Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2021-10-P-tuning v2":{"id":"Paper/NLP/PEFT/Soft Prompt/2021-10-P-tuning v2","title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2022-05-ATTEMPT":{"id":"Paper/NLP/PEFT/Soft Prompt/2022-05-ATTEMPT","title":"ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2022-05-SPoT":{"id":"Paper/NLP/PEFT/Soft Prompt/2022-05-SPoT","title":"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2022-12-XPrompt":{"id":"Paper/NLP/PEFT/Soft Prompt/2022-12-XPrompt","title":"XPrompt: Exploring the Extreme of Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-03-LLaMA-Adapter":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-03-LLaMA-Adapter","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-03-MPT":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-03-MPT","title":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-04-LLaMA-Adapter V2":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-04-LLaMA-Adapter V2","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-05-Residual-Prompt-Tuning":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-05-Residual-Prompt-Tuning","title":"Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-09-DEPT":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-09-DEPT","title":"DEPT: Decomposed Prompt Tuning For Parameter-Efficient Fine Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-12-APrompt":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-12-APrompt","title":"APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-12-SMoP":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-12-SMoP","title":"SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Prompt Tuning/2022-11-PTR":{"id":"Paper/NLP/Prompt Tuning/2022-11-PTR","title":"PTR: Prompt Tuning with Rules for Text Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","sidebar":"tutorialSidebar"},"Paper/NLP/Reinforcement Learning/2023-03-Reflexion":{"id":"Paper/NLP/Reinforcement Learning/2023-03-Reflexion","title":"Reflexion: Language Agents with Verbal Reinforcement Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Text Generation/2022-03-InstructGPT":{"id":"Paper/NLP/Text Generation/2022-03-InstructGPT","title":"Training language models to follow instructions with human feedback (+ ChatGPT)","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Survey/2021-07-Prompting":{"id":"Paper/Survey/2021-07-Prompting","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Survey/2024-02-PEFT Vision":{"id":"Paper/Survey/2024-02-PEFT Vision","title":"Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/2022-04-UPL":{"id":"Paper/Vision-Language/2022-04-UPL","title":"Unsupervised Prompt Learning for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/2022-12-Img2LLM":{"id":"Paper/Vision-Language/2022-12-Img2LLM","title":"From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/2023-03-Prismer":{"id":"Paper/Vision-Language/2023-03-Prismer","title":"Prismer: A Vision-Language Model with An Esemble of Experts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Contrastive Learning/2021-02-ALIGN":{"id":"Paper/Vision-Language/Contrastive Learning/2021-02-ALIGN","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision","description":"\ub17c\ubb38 \ubc0f image  \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Contrastive Learning/2021-03-CLIP":{"id":"Paper/Vision-Language/Contrastive Learning/2021-03-CLIP","title":"Learning Transferable Visual Models From Natural Language Supervision","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Multi-Modality/2023-12-AMMPL":{"id":"Paper/Vision-Language/PEFT/Multi-Modality/2023-12-AMMPL","title":"Adaptive Multi-Modality Prompt Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2021-02-CMAR":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2021-02-CMAR","title":"Cross-modal Adversarial Reprogramming","description":"\uc774\ubbf8\uc9c0 \ubc0f \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-03-VP":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-03-VP","title":"Exploring Visual Prompts for Adapting Large-Scale Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-10-Watermarking":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-10-Watermarking","title":"Watermarking for Out-of-distribution Detection","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-11-ILM-VP":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-11-ILM-VP","title":"Understanding and Improving Visual Prompting: A Label-Mapping Perspective","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-12-EVP":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-12-EVP","title":"Unleashing the Power of Visual Prompting At the Pixel Level","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-03-DAM-VP":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-03-DAM-VP","title":"Diversity-Aware Meta Visual Prompting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-03-EVP-L":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-03-EVP-L","title":"Explicit Visual Prompting for Low-Level Structure Segmentations","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-04-HintAug":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-04-HintAug","title":"Hint-Aug: Drawing Hints from Foundation Vision Transformers towards Boosted Few-shot Parameter-Efficient Tuning","description":"\ub17c\ubb38 \ubc0f image \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-10-AutoVP":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-10-AutoVP","title":"AutoVP: An Automated Visual Prompting Framework and Benchmark","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-11-A2XP":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-11-A2XP","title":"A2XP: Towards Private Domain Generalization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-11-InMeMo":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-11-InMeMo","title":"Instruct Me More! Random Prompting for Visual In-Context Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-12-LaViP":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-12-LaViP","title":"LaViP: Language-Grounded Visual Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-03-ADAVIPRO":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-03-ADAVIPRO","title":"AdaViPro: Region-Based Adaptive Visual Prompt For Large-Scale Models Adapting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-04-TVP":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-04-TVP","title":"Exploring the Transferability of Visual Prompting for Multimodal Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-06-SMM":{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-06-SMM","title":"Sample-specific Masks for Visual Reprogramming-based Prompting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-03-VPT":{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-03-VPT","title":"Visual Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-10-DePT":{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-10-DePT","title":"Visual Prompt Tuning For Test-time Domain Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-10-LPT":{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-10-LPT","title":"LPT: Long-Tailed Prompt Tuning For Image Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-03-CVP":{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-03-CVP","title":"Convolutional Visual Prompt for Robust Visual Perception","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-07-E2VPT":{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-07-E2VPT","title":"E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-12-SA2VP":{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-12-SA2VP","title":"SA^2VPT: Spatially Aligned-and-Adapted Visual Prompt","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Single-Stream/2019-08-VisualBERT":{"id":"Paper/Vision-Language/Single-Stream/2019-08-VisualBERT","title":"VisualBERT: A Simple And Performance Baseline For Visual And Language","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Single-Stream/2019-08-VLBERT":{"id":"Paper/Vision-Language/Single-Stream/2019-08-VLBERT","title":"VL-BERT: Pre-Training Of Generic Visual-Linguistic Representations","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Single-Stream/2019-09-UNITER":{"id":"Paper/Vision-Language/Single-Stream/2019-09-UNITER","title":"UNITER: UNiversal Image-TExt Representation Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Single-Stream/2020-01-ImageBERT":{"id":"Paper/Vision-Language/Single-Stream/2020-01-ImageBERT","title":"ImageBERT: Cross-Modal Pre-Training with Large-Scale Weak-Supervised Image-Text Data","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Single-Stream/2020-04-Pixel-BERT":{"id":"Paper/Vision-Language/Single-Stream/2020-04-Pixel-BERT","title":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers","description":"\uc800\uc790\ub294 unified end-to-end framework \ub85c visual \ubc0f language embedding \uc744 jointly learning \ud558\ub294 deep multi-modal transformer \ub97c \ud1b5\ud574 image pixels \ub97c text \uc640 align \ud558\ub294 Pixel-BERT \uc81c\uc548","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Single-Stream/2020-04-VD-BERT":{"id":"Paper/Vision-Language/Single-Stream/2020-04-VD-BERT","title":"VD-BERT: A Unified Vision and Dialog Transformer with BERT","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Two-Stream/2019-08-LXMERT":{"id":"Paper/Vision-Language/Two-Stream/2019-08-LXMERT","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Two-Stream/2019-08-ViLBERT":{"id":"Paper/Vision-Language/Two-Stream/2019-08-ViLBERT","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Programming/Algorithm/Algorithm":{"id":"Programming/Algorithm/Algorithm","title":"What is the Algorithm?","description":"\ubb38\uc81c \ud574\uacb0\uc744 \uc704\ud55c \uc808\ucc28\ub098 \ubc29\ubc95","sidebar":"tutorialSidebar"},"Programming/Algorithm/Problems":{"id":"Programming/Algorithm/Problems","title":"BackJoon","description":"","sidebar":"tutorialSidebar"},"tutorial-basics/congratulations":{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template.","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-blog-post":{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-document":{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-page":{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:","sidebar":"tutorialSidebar"},"tutorial-basics/deploy-your-site":{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack).","sidebar":"tutorialSidebar"},"tutorial-basics/markdown-features":{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features.","sidebar":"tutorialSidebar"},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs.","sidebar":"tutorialSidebar"},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let\'s translate docs/intro.md to French.","sidebar":"tutorialSidebar"}}}')}}]);