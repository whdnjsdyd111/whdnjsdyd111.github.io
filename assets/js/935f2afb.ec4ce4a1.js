"use strict";(self.webpackChunkwyj_lab=self.webpackChunkwyj_lab||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Tutorial Intro","href":"/docs/intro","docId":"intro"},{"type":"category","label":"Tutorial - Basics","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Create a Page","href":"/docs/tutorial-basics/create-a-page","docId":"tutorial-basics/create-a-page"},{"type":"link","label":"Create a Document","href":"/docs/tutorial-basics/create-a-document","docId":"tutorial-basics/create-a-document"},{"type":"link","label":"Create a Blog Post","href":"/docs/tutorial-basics/create-a-blog-post","docId":"tutorial-basics/create-a-blog-post"},{"type":"link","label":"Markdown Features","href":"/docs/tutorial-basics/markdown-features","docId":"tutorial-basics/markdown-features"},{"type":"link","label":"Deploy your site","href":"/docs/tutorial-basics/deploy-your-site","docId":"tutorial-basics/deploy-your-site"},{"type":"link","label":"Congratulations!","href":"/docs/tutorial-basics/congratulations","docId":"tutorial-basics/congratulations"}],"href":"/docs/category/tutorial---basics"},{"type":"category","label":"Tutorial - Extras","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Manage Docs Versions","href":"/docs/tutorial-extras/manage-docs-versions","docId":"tutorial-extras/manage-docs-versions"},{"type":"link","label":"Translate your site","href":"/docs/tutorial-extras/translate-your-site","docId":"tutorial-extras/translate-your-site"}],"href":"/docs/category/tutorial---extras"},{"type":"category","label":"Paper","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Computer Vision","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Image Classification","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"An Image Is Worth 16X16 Words: Transformers for image recognition at sacle","href":"/docs/Paper/Computer Vision/Image Classification/ViT","docId":"Paper/Computer Vision/Image Classification/2020-10-ViT"},{"type":"link","label":"EfficientNetV2: Smaller Models and Faster Training","href":"/docs/Paper/Computer Vision/Image Classification/EfficientNetV2","docId":"Paper/Computer Vision/Image Classification/2021-04-EfficientNetV2"}]},{"type":"category","label":"Multi-task","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"A Unified Sequence Interface for Vision Tasks","href":"/docs/Paper/Computer Vision/Multi-task/Unified Interface","docId":"Paper/Computer Vision/Multi-task/2022-06-Unified Interface"},{"type":"link","label":"UNINEXT: Universal Instance Perception as Object Discovery and Retrieval","href":"/docs/Paper/Computer Vision/Multi-task/UNINEXT","docId":"Paper/Computer Vision/Multi-task/2023-03-UNINEXT"}]},{"type":"category","label":"Vision-Language","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Unsupervised Prompt Learning for Vision-Language Models","href":"/docs/Paper/Computer Vision/Vision-Language/UPL","docId":"Paper/Computer Vision/Vision-Language/2022-04-UPL"},{"type":"link","label":"From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models","href":"/docs/Paper/Computer Vision/Vision-Language/Img2LLM","docId":"Paper/Computer Vision/Vision-Language/2022-12-Img2LLM"},{"type":"link","label":"Prismer: A Vision-Language Model with An Esemble of Experts","href":"/docs/Paper/Computer Vision/Vision-Language/Prismer","docId":"Paper/Computer Vision/Vision-Language/2023-03-Prismer"}]}]},{"type":"category","label":"NLP","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Multi-Task","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","href":"/docs/Paper/NLP/Multi-Task/Chain-of-Thought","docId":"Paper/NLP/Multi-Task/2022-01-CoT"},{"type":"link","label":"Scaling Instruction-Finetuned Language Models","href":"/docs/Paper/NLP/Multi-Task/Flan-T5","docId":"Paper/NLP/Multi-Task/2022-10-Flan-T5"},{"type":"link","label":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation","href":"/docs/Paper/NLP/Multi-Task/CodeT5+","docId":"Paper/NLP/Multi-Task/2023-05-CodeT5p"}]},{"type":"category","label":"PEFT","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Prefix-Tuning: Optimizing Continuous Prompts for Generation","href":"/docs/Paper/NLP/PEFT/Prefix-Tuning","docId":"Paper/NLP/PEFT/2021-01-Prefix-Tuning"},{"type":"link","label":"GPT Understands, Too","href":"/docs/Paper/NLP/PEFT/P-tuning","docId":"Paper/NLP/PEFT/2021-03-P-tuning"},{"type":"link","label":"The Power of Scale for Parameter-Efficient Prompt Tuning","href":"/docs/Paper/NLP/PEFT/Prompt Tuning","docId":"Paper/NLP/PEFT/2021-04-Prompt-Tuning"},{"type":"link","label":"LoRA: Low-Rank Adaptation of Large Language Models","href":"/docs/Paper/NLP/PEFT/LoRA","docId":"Paper/NLP/PEFT/2021-06-LoRA"},{"type":"link","label":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks","href":"/docs/Paper/NLP/PEFT/P-tuning v2","docId":"Paper/NLP/PEFT/2021-10-P-tuning v2"},{"type":"link","label":"ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts","href":"/docs/Paper/NLP/PEFT/ATTEMPT","docId":"Paper/NLP/PEFT/2022-05-ATTEMPT"},{"type":"link","label":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning","href":"/docs/Paper/NLP/PEFT/IA\xb3","docId":"Paper/NLP/PEFT/2022-05-IA\xb3"},{"type":"link","label":"Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning","href":"/docs/Paper/NLP/PEFT/AdaLoRA","docId":"Paper/NLP/PEFT/2023-03-AdaLoRA"},{"type":"link","label":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","href":"/docs/Paper/NLP/PEFT/LLaMA-Adapter","docId":"Paper/NLP/PEFT/2023-03-LLaMA-Adapter"},{"type":"link","label":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning","href":"/docs/Paper/NLP/PEFT/Multitask Prompt Tuning","docId":"Paper/NLP/PEFT/2023-03-MPT"},{"type":"link","label":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","href":"/docs/Paper/NLP/PEFT/LLaMA-Adapter V2","docId":"Paper/NLP/PEFT/2023-04-LLaMA-Adapter V2"},{"type":"link","label":"Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization","href":"/docs/Paper/NLP/PEFT/Residual Prompt Tuning","docId":"Paper/NLP/PEFT/2023-05-Residual-Prompt-Tuning"},{"type":"link","label":"DEPT: Decomposed Prompt Tuning For Parameter-Efficient Fine Tuning","href":"/docs/Paper/NLP/PEFT/DEPT","docId":"Paper/NLP/PEFT/2023-09-DEPT"}]},{"type":"category","label":"Prompt Tuning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"PTR: Prompt Tuning with Rules for Text Classification","href":"/docs/Paper/NLP/Prompt Tuning/PTR","docId":"Paper/NLP/Prompt Tuning/2022-11-PTR"}]},{"type":"category","label":"Reinforcement Learning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Reflexion: Language Agents with Verbal Reinforcement Learning","href":"/docs/Paper/NLP/Reinforcement Learning/Reflexion","docId":"Paper/NLP/Reinforcement Learning/2023-03-Reflexion"}]},{"type":"category","label":"Survey","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","href":"/docs/Paper/NLP/Survey/Prompting","docId":"Paper/NLP/Survey/2021-07-Prompting"}]},{"type":"category","label":"Text Generation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Training language models to follow instructions with human feedback (+ ChatGPT)","href":"/docs/Paper/NLP/Text Generation/InstructGPT","docId":"Paper/NLP/Text Generation/2022-03-InstructGPT"}]}]}]}]},"docs":{"intro":{"id":"intro","title":"Tutorial Intro","description":"Let\'s discover Docusaurus in less than 5 minutes.","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Image Classification/2020-10-ViT":{"id":"Paper/Computer Vision/Image Classification/2020-10-ViT","title":"An Image Is Worth 16X16 Words: Transformers for image recognition at sacle","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Image Classification/2021-04-EfficientNetV2":{"id":"Paper/Computer Vision/Image Classification/2021-04-EfficientNetV2","title":"EfficientNetV2: Smaller Models and Faster Training","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Multi-task/2022-06-Unified Interface":{"id":"Paper/Computer Vision/Multi-task/2022-06-Unified Interface","title":"A Unified Sequence Interface for Vision Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Multi-task/2023-03-UNINEXT":{"id":"Paper/Computer Vision/Multi-task/2023-03-UNINEXT","title":"UNINEXT: Universal Instance Perception as Object Discovery and Retrieval","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Vision-Language/2022-04-UPL":{"id":"Paper/Computer Vision/Vision-Language/2022-04-UPL","title":"Unsupervised Prompt Learning for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Vision-Language/2022-12-Img2LLM":{"id":"Paper/Computer Vision/Vision-Language/2022-12-Img2LLM","title":"From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Vision-Language/2023-03-Prismer":{"id":"Paper/Computer Vision/Vision-Language/2023-03-Prismer","title":"Prismer: A Vision-Language Model with An Esemble of Experts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Multi-Task/2022-01-CoT":{"id":"Paper/NLP/Multi-Task/2022-01-CoT","title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Multi-Task/2022-10-Flan-T5":{"id":"Paper/NLP/Multi-Task/2022-10-Flan-T5","title":"Scaling Instruction-Finetuned Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Multi-Task/2023-05-CodeT5p":{"id":"Paper/NLP/Multi-Task/2023-05-CodeT5p","title":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2021-01-Prefix-Tuning":{"id":"Paper/NLP/PEFT/2021-01-Prefix-Tuning","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2021-03-P-tuning":{"id":"Paper/NLP/PEFT/2021-03-P-tuning","title":"GPT Understands, Too","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2021-04-Prompt-Tuning":{"id":"Paper/NLP/PEFT/2021-04-Prompt-Tuning","title":"The Power of Scale for Parameter-Efficient Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2021-06-LoRA":{"id":"Paper/NLP/PEFT/2021-06-LoRA","title":"LoRA: Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2021-10-P-tuning v2":{"id":"Paper/NLP/PEFT/2021-10-P-tuning v2","title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2022-05-ATTEMPT":{"id":"Paper/NLP/PEFT/2022-05-ATTEMPT","title":"ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2022-05-IA\xb3":{"id":"Paper/NLP/PEFT/2022-05-IA\xb3","title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2023-03-AdaLoRA":{"id":"Paper/NLP/PEFT/2023-03-AdaLoRA","title":"Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2023-03-LLaMA-Adapter":{"id":"Paper/NLP/PEFT/2023-03-LLaMA-Adapter","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2023-03-MPT":{"id":"Paper/NLP/PEFT/2023-03-MPT","title":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2023-04-LLaMA-Adapter V2":{"id":"Paper/NLP/PEFT/2023-04-LLaMA-Adapter V2","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2023-05-Residual-Prompt-Tuning":{"id":"Paper/NLP/PEFT/2023-05-Residual-Prompt-Tuning","title":"Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2023-09-DEPT":{"id":"Paper/NLP/PEFT/2023-09-DEPT","title":"DEPT: Decomposed Prompt Tuning For Parameter-Efficient Fine Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Prompt Tuning/2022-11-PTR":{"id":"Paper/NLP/Prompt Tuning/2022-11-PTR","title":"PTR: Prompt Tuning with Rules for Text Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","sidebar":"tutorialSidebar"},"Paper/NLP/Reinforcement Learning/2023-03-Reflexion":{"id":"Paper/NLP/Reinforcement Learning/2023-03-Reflexion","title":"Reflexion: Language Agents with Verbal Reinforcement Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Survey/2021-07-Prompting":{"id":"Paper/NLP/Survey/2021-07-Prompting","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Text Generation/2022-03-InstructGPT":{"id":"Paper/NLP/Text Generation/2022-03-InstructGPT","title":"Training language models to follow instructions with human feedback (+ ChatGPT)","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"tutorial-basics/congratulations":{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template.","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-blog-post":{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-document":{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-page":{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:","sidebar":"tutorialSidebar"},"tutorial-basics/deploy-your-site":{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack).","sidebar":"tutorialSidebar"},"tutorial-basics/markdown-features":{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features.","sidebar":"tutorialSidebar"},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs.","sidebar":"tutorialSidebar"},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let\'s translate docs/intro.md to French.","sidebar":"tutorialSidebar"}}}')}}]);