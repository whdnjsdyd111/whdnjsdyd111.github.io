"use strict";(self.webpackChunkwyj_lab=self.webpackChunkwyj_lab||[]).push([[1992],{65325:o=>{o.exports=JSON.parse('{"label":"Low-Rank","permalink":"/docs/tags/low-rank","allTagsPath":"/docs/tags","count":18,"items":[{"id":"Paper/NLP/PEFT/Composition/2023-03-AdaLoRA","title":"Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/AdaLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-03-BiLoRA","title":"BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/BiLoRA"},{"id":"Paper/NLP/PEFT/Composition/2023-10-SaLoRA","title":"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/SaLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-01-COLA","title":"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/COLA"},{"id":"Paper/NLP/PEFT/Composition/2023-09-Delta-LoRA","title":"Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/Delta-LoRA"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-09-DEPT","title":"DEPT: Decomposed Prompt Tuning For Parameter-Efficient Fine Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/DEPT"},{"id":"Paper/NLP/PEFT/Composition/2022-10-DyLoRA","title":"DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/DyLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-02-FLoRA","title":"FLoRA: Low-Rank Adapters Are Secretly Gradient Compressors","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/FLoRA"},{"id":"Paper/NLP/PEFT/Composition/2023-08-IncreLoRA","title":"IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/IncreLoRA"},{"id":"Paper/Computer Vision/PEFT/Composition/2024-04-InfLoRA","title":"InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Computer Vision/PEFT/Composition/InfLoRA"},{"id":"Paper/NLP/PEFT/Composition/2023-09-LongLoRA","title":"LongLoRA: Efficient Fine-Tuning of LongContext Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LongLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-04-LoRA-Drop","title":"LoRA Dropout as a Sparsity Regularizer for Overfitting Control","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoRA-Dropout"},{"id":"Paper/NLP/PEFT/Composition/2021-06-LoRA","title":"LoRA: Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoRA"},{"id":"Paper/Vision-Language/PEFT/Composition/2024-05-CLIP-LoRA","title":"Low-Rank Few-Shot Adaptation of Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Composition/CLIP-LoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-02-MeLoRA","title":"MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/MeLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-05-MoRA","title":"MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/MoRA"},{"id":"Paper/NLP/PEFT/Composition/2023-07-ReLoRA","title":"ReLoRA: High-Rank Training Through Low-Rank Updates","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ReLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-06-SinkLoRA","title":"SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/SinkLoRA"}]}')}}]);