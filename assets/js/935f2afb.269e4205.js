"use strict";(self.webpackChunkwyj_lab=self.webpackChunkwyj_lab||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Tutorial Intro","href":"/docs/intro","docId":"intro"},{"type":"category","label":"Tutorial - Basics","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Create a Page","href":"/docs/tutorial-basics/create-a-page","docId":"tutorial-basics/create-a-page"},{"type":"link","label":"Create a Document","href":"/docs/tutorial-basics/create-a-document","docId":"tutorial-basics/create-a-document"},{"type":"link","label":"Create a Blog Post","href":"/docs/tutorial-basics/create-a-blog-post","docId":"tutorial-basics/create-a-blog-post"},{"type":"link","label":"Markdown Features","href":"/docs/tutorial-basics/markdown-features","docId":"tutorial-basics/markdown-features"},{"type":"link","label":"Deploy your site","href":"/docs/tutorial-basics/deploy-your-site","docId":"tutorial-basics/deploy-your-site"},{"type":"link","label":"Congratulations!","href":"/docs/tutorial-basics/congratulations","docId":"tutorial-basics/congratulations"}],"href":"/docs/category/tutorial---basics"},{"type":"category","label":"Tutorial - Extras","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Manage Docs Versions","href":"/docs/tutorial-extras/manage-docs-versions","docId":"tutorial-extras/manage-docs-versions"},{"type":"link","label":"Translate your site","href":"/docs/tutorial-extras/translate-your-site","docId":"tutorial-extras/translate-your-site"}],"href":"/docs/category/tutorial---extras"},{"type":"category","label":"Paper","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Computer Vision","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Image Classification","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Paper Summarize EfficientNetV2","href":"/docs/Paper/Computer Vision/Image Classification/EfficientNetV2","docId":"Paper/Computer Vision/Image Classification/2023-03-12-EfficientNetV2"},{"type":"link","label":"An Image Is Worth 16X16 Words: Transformers for image recognition at sacle","href":"/docs/Paper/Computer Vision/Image Classification/ViT","docId":"Paper/Computer Vision/Image Classification/2023-03-17-ViT"}]},{"type":"category","label":"Multi-task","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"UNINEXT: Universal Instance Perception as Object Discovery and Retrieval","href":"/docs/Paper/Computer Vision/Multi-task/UNINEXT","docId":"Paper/Computer Vision/Multi-task/2023-04-07-UNINEXT"},{"type":"link","label":"A Unified Sequence Interface for Vision Tasks","href":"/docs/Paper/Computer Vision/Multi-task/Unified Interface","docId":"Paper/Computer Vision/Multi-task/2023-05-08-Unified Interface"}]},{"type":"category","label":"Vision-Language","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Prismer: A Vision-Language Model with An Esemble of Experts","href":"/docs/Paper/Computer Vision/Vision-Language/Prismer","docId":"Paper/Computer Vision/Vision-Language/2023-03-29-Prismer"},{"type":"link","label":"From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models","href":"/docs/Paper/Computer Vision/Vision-Language/Img2LLM","docId":"Paper/Computer Vision/Vision-Language/2023-05-21-Img2LLM"},{"type":"link","label":"Unsupervised Prompt Learning for Vision-Language Models","href":"/docs/Paper/Computer Vision/Vision-Language/UPL","docId":"Paper/Computer Vision/Vision-Language/2023-05-23-UPL"}]}]},{"type":"category","label":"NLP","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Multi-Task","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation","href":"/docs/Paper/NLP/Multi-Task/CodeT5+","docId":"Paper/NLP/Multi-Task/2023-07-15-CodeT5p"},{"type":"link","label":"Scaling Instruction-Finetuned Language Models","href":"/docs/Paper/NLP/Multi-Task/Flan-T5","docId":"Paper/NLP/Multi-Task/2023-08-25-Flan-T5"},{"type":"link","label":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","href":"/docs/Paper/NLP/Multi-Task/Chain-of-Thought","docId":"Paper/NLP/Multi-Task/2023-08-29-CoT"}]},{"type":"category","label":"PEFT","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning","href":"/docs/Paper/NLP/PEFT/Multitask Prompt Tuning","docId":"Paper/NLP/PEFT/2023-08-31-MPT"},{"type":"link","label":"LoRA: Low-Rank Adaptation of Large Language Models","href":"/docs/Paper/NLP/PEFT/LoRA","docId":"Paper/NLP/PEFT/2023-09-06-LoRA"},{"type":"link","label":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","href":"/docs/Paper/NLP/PEFT/LLaMA-Adapter","docId":"Paper/NLP/PEFT/2023-09-08-LLaMA-Adapter"},{"type":"link","label":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","href":"/docs/Paper/NLP/PEFT/LLaMA-Adapter V2","docId":"Paper/NLP/PEFT/2023-09-09-LLaMA-Adapter V2"},{"type":"link","label":"GPT Understands, Too","href":"/docs/Paper/NLP/PEFT/P-tuning","docId":"Paper/NLP/PEFT/2023-09-11-P-tuning"}]},{"type":"category","label":"Reinforcement Learning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Reflexion: Language Agents with Verbal Reinforcement Learning","href":"/docs/Paper/NLP/Reinforcement Learning/Reflexion","docId":"Paper/NLP/Reinforcement Learning/2023-06-27-Reflexion"}]},{"type":"category","label":"Survey","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","href":"/docs/Paper/NLP/Survey/Prompting","docId":"Paper/NLP/Survey/2023-05-31-Prompting"}]},{"type":"category","label":"Text Generation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Training language models to follow instructions with human feedback (+ ChatGPT)","href":"/docs/Paper/NLP/Text Generation/InstructGPT","docId":"Paper/NLP/Text Generation/2023-03-24-InstructGPT"}]}]}]}]},"docs":{"intro":{"id":"intro","title":"Tutorial Intro","description":"Let\'s discover Docusaurus in less than 5 minutes.","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Image Classification/2023-03-12-EfficientNetV2":{"id":"Paper/Computer Vision/Image Classification/2023-03-12-EfficientNetV2","title":"Paper Summarize EfficientNetV2","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Image Classification/2023-03-17-ViT":{"id":"Paper/Computer Vision/Image Classification/2023-03-17-ViT","title":"An Image Is Worth 16X16 Words: Transformers for image recognition at sacle","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Multi-task/2023-04-07-UNINEXT":{"id":"Paper/Computer Vision/Multi-task/2023-04-07-UNINEXT","title":"UNINEXT: Universal Instance Perception as Object Discovery and Retrieval","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Multi-task/2023-05-08-Unified Interface":{"id":"Paper/Computer Vision/Multi-task/2023-05-08-Unified Interface","title":"A Unified Sequence Interface for Vision Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Vision-Language/2023-03-29-Prismer":{"id":"Paper/Computer Vision/Vision-Language/2023-03-29-Prismer","title":"Prismer: A Vision-Language Model with An Esemble of Experts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Vision-Language/2023-05-21-Img2LLM":{"id":"Paper/Computer Vision/Vision-Language/2023-05-21-Img2LLM","title":"From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Vision-Language/2023-05-23-UPL":{"id":"Paper/Computer Vision/Vision-Language/2023-05-23-UPL","title":"Unsupervised Prompt Learning for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Multi-Task/2023-07-15-CodeT5p":{"id":"Paper/NLP/Multi-Task/2023-07-15-CodeT5p","title":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Multi-Task/2023-08-25-Flan-T5":{"id":"Paper/NLP/Multi-Task/2023-08-25-Flan-T5","title":"Scaling Instruction-Finetuned Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Multi-Task/2023-08-29-CoT":{"id":"Paper/NLP/Multi-Task/2023-08-29-CoT","title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2023-08-31-MPT":{"id":"Paper/NLP/PEFT/2023-08-31-MPT","title":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2023-09-06-LoRA":{"id":"Paper/NLP/PEFT/2023-09-06-LoRA","title":"LoRA: Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2023-09-08-LLaMA-Adapter":{"id":"Paper/NLP/PEFT/2023-09-08-LLaMA-Adapter","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2023-09-09-LLaMA-Adapter V2":{"id":"Paper/NLP/PEFT/2023-09-09-LLaMA-Adapter V2","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/2023-09-11-P-tuning":{"id":"Paper/NLP/PEFT/2023-09-11-P-tuning","title":"GPT Understands, Too","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Reinforcement Learning/2023-06-27-Reflexion":{"id":"Paper/NLP/Reinforcement Learning/2023-06-27-Reflexion","title":"Reflexion: Language Agents with Verbal Reinforcement Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Survey/2023-05-31-Prompting":{"id":"Paper/NLP/Survey/2023-05-31-Prompting","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Text Generation/2023-03-24-InstructGPT":{"id":"Paper/NLP/Text Generation/2023-03-24-InstructGPT","title":"Training language models to follow instructions with human feedback (+ ChatGPT)","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"tutorial-basics/congratulations":{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template.","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-blog-post":{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-document":{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-page":{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:","sidebar":"tutorialSidebar"},"tutorial-basics/deploy-your-site":{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack).","sidebar":"tutorialSidebar"},"tutorial-basics/markdown-features":{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features.","sidebar":"tutorialSidebar"},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs.","sidebar":"tutorialSidebar"},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let\'s translate docs/intro.md to French.","sidebar":"tutorialSidebar"}}}')}}]);