"use strict";(self.webpackChunkwyj_lab=self.webpackChunkwyj_lab||[]).push([[9045],{21421:i=>{i.exports=JSON.parse('{"label":"prompt tuning","permalink":"/docs/tags/prompt-tuning","allTagsPath":"/docs/tags","count":11,"items":[{"id":"Paper/NLP/PEFT/2023-12-APrompt","title":"APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/APrompt"},{"id":"Paper/NLP/PEFT/2022-05-ATTEMPT","title":"ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/ATTEMPT"},{"id":"Paper/NLP/PEFT/2023-09-DEPT","title":"DEPT: Decomposed Prompt Tuning For Parameter-Efficient Fine Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/DEPT"},{"id":"Paper/NLP/PEFT/2021-10-P-tuning v2","title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/P-tuning v2"},{"id":"Paper/NLP/PEFT/2021-01-Prefix-Tuning","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Prefix-Tuning"},{"id":"Paper/NLP/Prompt Tuning/2022-11-PTR","title":"PTR: Prompt Tuning with Rules for Text Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","permalink":"/docs/Paper/NLP/Prompt Tuning/PTR"},{"id":"Paper/NLP/PEFT/2023-05-Residual-Prompt-Tuning","title":"Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","permalink":"/docs/Paper/NLP/PEFT/Residual Prompt Tuning"},{"id":"Paper/NLP/PEFT/2023-12-SMoP","title":"SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts","description":"\ub17c\ubb38 \uc774\ubbf8\uc9c0 \ubc0f \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/SMoP"},{"id":"Paper/NLP/PEFT/2022-05-SPoT","title":"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/SPoT"},{"id":"Paper/NLP/PEFT/2021-04-Prompt-Tuning","title":"The Power of Scale for Parameter-Efficient Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Prompt Tuning"},{"id":"Paper/NLP/PEFT/2022-12-XPrompt","title":"XPrompt: Exploring the Extreme of Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/XPrompt"}]}')}}]);