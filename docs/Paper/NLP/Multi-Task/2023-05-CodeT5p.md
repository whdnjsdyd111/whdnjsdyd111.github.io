---
slug: CodeT5+
title: "CodeT5+: Open Code Large Language Models for Code Understanding and Generation"
tags: [CodeT5p, CodeT5+, CodeT5, NLP, Code Understanding, Code Generation, Multi-Task, Paper]
---

논문 및 이미지 출처 : <https://arxiv.org/pdf/2305.07922v2.pdf>

# Abstract

현재 code LLMs 는 두 가지 주요 한계점 존재

1. specific architecture (encoder-only, decoder-only) 또는 encoder-decoder network 에 의존
    - 전자는 응용하는데 있어 inflexibility
    - 후자는 모든 task 에 대한 single system 을 다루어 subset 에 대해 suboptimal 성능을 보임
2. 관련없는 downstream task 로 pretraining limited set 을 사용

이를 해결하기 위해, component modules 를 유연하게 결합하여 넓은 범위의 downstream code task 에 적합한 encoder-decoder LLMs 의 **CodeT5+** 를 제안

이러한 유연성은 pretrain-finetune 불일치성을 완화하기 위해 **pretraining objectives 의 mixture**을 제안

이 objectives 는 단일 또는 이중의 code 말뭉치에서 span denoising, contrasive learning, text-code matching 및 causal LM pretraining tasks 수행 가능

또한 모델을 효율적으로 scale up 하기 위해 CodeT5+ 를 처음부터 훈련하지 않고 frozen off-the-shelf LLMs 으로 초기화하는 방법

그리고 instruction-tuning 을 탐구

저자는 CodeT5+ 를 20개의 code-related benchmarks 에 광범위하게 평가를 했으며, zero-shot, finetuning, instruction-tuning 을 포함한다.

code generation/completion, math programming 및 text-to-code retrieval task 같은 다양한 code-related task 에서 SOTA 를 달성


# 1. Introduction

LLMs 는 대규모 코드 기반 데이터 (예; GitHub 공개 데이터)로 pretrain 하여 다양한 코드 관련 downstream task 로 transfer 할 수 있다.

하지만 기존의 많은 모델이 특정 downstream task 만 잘 수행되도록 설계가 되어 있다. 이는 주로 아키텍처와 pretraining task 수행 과제에 대한 두 제한으로 인한 것으로 주장한다.

<hr/>

**Architecture**

![Figure 1](image.png)

기존의 code LLM 은 understanding / generation task 에만 잘 수행되는 encoder-only / decoder-only 모델을 채택한다.

특히, text-to-code retrieval 같은 understading task 용이한 encoder model,
code generation 같은 generation task 에 대한 decoder model 이 강력한 성능을 보인다.

그러나 디코더 모델은 인코더 모델에 비해 검색 및 감지 작업같은 understading task 에 이상적이지 않으며, 최근 encoder-decoder architecture 를 많이 채택한다.

understanding 과 generation 모두를 지원하지만, 여전히 최적의 성능을 내지 못한다.

Unixcoder 는 encoder-decoder 모델이 검색 및 코드 완성 작업에서 SOTA 인 encoder 및 decoder-only 모델을 능가하지 못하는 것을 발견했다.

이 결함은 주로 모든 작업에 적응되는 단일 모듈 아키텍처의 한계다. 요약하자면, 기존 접근방식은 개별 구성 요소가 다른 downstream task 에 더 활성화될 수 있도록 설계되지 않았다.

<hr/>

현재 제한된 훈련셋으로 사용하여, pretrain, transfer 사이의 불일치로 인한 downstream task 의 성능 하락을 야기함.

예로 T5 기반 모델은 종종 span denoising 목적으로 훈련된다. 그러나 코드 생성 같은 downstream task 의 대부분 SOTA 모델은 프로그램 토큰을 하나씩 auto-regressively predict 하여 다음 토큰을 예측하는 목적으로 pretrain 한다.

최근 시도는 위 문제 완화를 위해 contrastive learning 을 도입하지만, text 와 code representation 사이의 alignment 를 무시한다.

<hr/>

저자는 CodeT5+ 의 모델 크기를 확장하기 위해 계산 효율적인 pretrain 전략을 사용하여 CodeT5+ 의 구성 요소를 초기화하는데 존재하는 code LLMs 를 활용한다.

pretrained checkpoint 로 encoder, decoder 를 초기화하고 cross attention layer 로 연결하는 "shallow encoder, deep decoder" architecture 를 채택한다.

deep decoder LLM 은 고정하며, shallow encoder 와 cross attention layer 만 훈련하여 효율적인 조정을 위해 훈련 가능한 매개 변수의 수를 크게 줄인다.

마지막으로, NLP 분야의 최근 연구인로 Instruction tuning 으로 탐구한다.

<hr/>

20개 이상의 code 관련 벤치마크에서 CodeT5+ 를 광범위하게 평가했다.

zero-shot, finetuning 및 instruction tuning 을 포함한다.

결과는 CodeT+ 가 많은 downstream 작업에서 SOTA baseline 에 비해 상당한 성능 향상을 보여준다.

# 2. Related Work



# 3. CodeT5+: Open Code Large Language Models

![](https://velog.velcdn.com/images/whdnjsdyd111/post/05132274-f16b-4877-8d32-6083f1b509ea/image.png)


code understanding 및 generation task 를 위한 새로운 open code LLMs 인 CodeT5+ 를 개발.

encoder-decoder 를 기반으로한 CodeT5+ 는 unimodal 및 bimodal 데이터에 대한 다양한 pretrain objectives 를 통해 다양한 모드에서 작동 가능한 유연성을 갖추고 있다.

1. unimodal pretraining 첫 단계로, 계산 효율적인 목적으로 대규모 코드 데이터로 pretrain
2. bimodal pretraining 두 번째 단계로, cross-modal 학습 목적으로 code-text data 의 smaller set으로 모델을 계속 pretrain
3. 각 단계에서 여러 pretrain objective 를 동일한 가중치로 공통 최적화

위 접근 방식이 모델이 다양한 데이터에 노출되어 풍부한 context representation 을 학습하는 데 효율적이란 것을 발견

![](https://velog.velcdn.com/images/whdnjsdyd111/post/0855f084-7dc0-4ab4-8ec3-8069254a01fc/image.png)

## 3.1. Unimodal Pretraining on Code Data

먼저, CodeT+ 를 대규모 코드 unimodal data 로 사전학습. GitHub 의 오픈 소스로, 코드 및 주석이 포함되어 있음.

두 번째로, code-text 쌍의 데이터로, Span Denoising 및 CLM task 를 사전학습한다. 이 작업은 모델이 다양한 범위의 code context 를 복구하는 방법을 학습할 수 있도록 한다.

### Span Denoising

encoder input 에 15% 토큰을 무작위 mask 로 대체하고, decoder 가 복구하도록 한다.

### Causal Language Modeling (CLM)

두 가지 변형으로 auto-regressive generation 을 위해 최적화.

1. 임의로 피벗 위치를 선택하여, 그 이전의 context 를 source sequence 로, 이 후의 시퀀스를 target output 으로 간주.
이를 seq2seq 와 언어 모델링 목적으로 표기
피벗 위치는 전체 sequence 의 10% - 90% 사이에서 균등하게 샘플링되도록 제한하고 source sequence 에 특수 토큰 [CLM] 을 추가
2. decoder 전용 generation task 로, 첫 번째 변형의 극단적인 경우.
[CLM] 토큰을 encoder 에 입력으로 전달하고, decoder 에게 전체 코드 시퀀스를 생성하도록 요구.

## 3.2. Bimodal Pretraining on Text-code Data

두 번째 단계로, 저자는 text-code bimodal data 로 pretrain 하였다.

여기서 각 text-code 쌍은 code function 과 대응하는 docstring describing 을 포함한다.

이런 bimodal data 는 모델 훈련이 cross-modal understanding 과 generation 에 용이하도록 한다.

bimodal tasks 는 cross-modal contrastive learning, matching 및 causal LM task 를 포함한다.

### Text-Code Contrastive Learning

이 tasks 는 text 및 code representations 의 feature space 를 정렬하기 위해 positive text-code paris 는 모으고 negative text-code pairs 를 분리하는 것을 목표로 한다.

Guo et al. [2022] 는 code understanding task 에서 이점을 입증했다. 이 task 는 encoder 에서만 활성화하며, text 나 code snippet 을 bidirectional self-attention 을 통해 continuous representation 으로 인코딩한다.

BERT 와 유사하게, 저자는 input 앞에 special token [CLS] 을 붙이고, 최종 Transformer layer 의 output embeddings 를 해당 input text 나 input code 의 representations 으로 간주한다.

또한, linear layer 를 추가하고 L2 normalization 을 사용하여 출력을 256-dimensional embeddings 로 매핑한다.

negative samples 를 보강하기 위해, momentum encoder 를 사용하여 이전 mini-batches 의 임베딩을 저장한다.

구체적으로, momentum encoder 는 현재 mini-batch 의 샘플을 enqueue 하고 가장 오래된 mini-batch 는 dequeue 하여 queuing 시스템을 유지한다.

기존의 encoder 와 momentum encoder 의 linear interpolation 을 통해 momentum encoder 를 업데이트하여 training step 간의 representation 일관성을 보장한다.

### Text-Code Matching

이 task 는 decoder 를 활성화시키고 text 와 code snippet 이 동일안 의미를 가지는지 예측하는 것을 목표로 함.

이는 text 와 code modalities 간의 fine-grained alignment 를 포착하는 더 나은 bimodal representations 를 학습 하는데 도움이 된다.

code sample 이 주어지면 decoder 는 embedding layer 및 causal self-attention layer 를 통과시킨 후, self-attention representation 은 cross attention layer 로 전달되어 encoder 로부터 받은 text representation 와 관련한 signal 을 query 한다.

task-specific [Match] token 은 code input sequence 의 맨 앞에 추가되어 decoder 에 text-code matching functionality 를 제공해주며, [EOS] token 은 code input 끝에 추가된다.

decoder 는 causal self-attention mask 를 사용하며 last decoder token 만 전체 context 에 참여할 수 있어, [EOS] 의 output embedding 을 text-code cross-modal 의 alignment representation 으로 다룬다.

마지막으로, binary matching task 를 위해 decoder 의 output embedding 위에 linear layer 를 사용하여 text-code pair 가 positive (match) 인지 negative (unmatched) 인지를 예측한다.

<hr/>

정보가 많은 negative 를 찾기 위해, 저자는 hard negative mining 전략을 사용한다.

특히, 현재 샘플과 momentum encoder 가 유지하는 queue 내의 이전 샘플 간의 contrastive-based similarity score 에 따라 hard negative 를 샘플링한다.

이렇게 하면 harder negative 가 선택될 가능성이 높아진다.

positive pairs 의 batch 의 경우, code/text query 를 사용하여 text/code queue 에서 negative 를 mining 하여 negative pairs 의 두 batch 를 구성한다.

### Text-Code Causal LM

이 task 는 encoder 및 decoder 모두 활성화시키며, text-to-code 및 code-to-text 생성으로 dual multimodal conversion 을 통한 cross-modal generative 목표에 초점을 둔다.

구체적으로, text sample 이 input 일 경우, decoder 의 input sequence 에 [CDec] token 을 앞에 추가한다. 

이 경우, decoder 는 code generation funtionality 로 작동한다. 

반대로, input 이 code sample 인 경우엔 decoder 의 input sequence dp [TDec] token 맨 앞에 추가하여, deocder 는 text generation functionality 로 작동한다.

이런 유형의 Causal LM 은 code summarization 같은 multimodal generative downstream tasks 에서 pretrain-finetune gap 을 줄이기 위한 효과적인 learning objective 로 입증되었다.

## 3.3. Compute-efficient Pretraining with Frozen Off-the-shelf LLMs

모델을 첨부터 pretraining 하지 않고 효율적으로 확장하기 위해, 저자는 CodeT5+ 의 component (encoder, decoder)를 off-the-shelf pretrained LLM 로 초기화하는 compute-efficient pretraining 전략을 제안한다. (Fig 2. 오른쪽)

이 확장을 위해, [Li et al., 2022b] 의 영감을 받아, 기존 T5 모델과 동일한 크기의 encoder 와 decoder 대신 "shallow encoder and deep decoder" architecture 를 사용한다.

[Li et al., 2022b] 에 따르면, T5-based model 의 decoder 는 종종 생성 작업에서 더 높은 복잡성을 처리해야 하므로, 더 많은 neural parameters 로 강화해야 한다.

분리되어 pretrain 된 encoder 와 decoder 를 연결하기 위해, 저자는 self-attention layer 이후 decoder block 에 무작위로 초기화된 cross-attention layer 를 삽입한다.

efficient tuning 을 위해, cross-attention layer 는 top-$L$ decoder layer (본 실험은$L$=1)에만 삽입한다.

small encoder 와 cross-attention layer 만 trainable 하도록 유지하면서, 대부분의 decoder parameter 는 고정한다. 또한, training stability 향상을 위해 gating function 을 추가하거나 특정 frequency 로 multiple cross-attention layer 를 삽입하는 등의 설계도 탐구한다.

하지만, 상당한 성능 향상은 관찰되지 않았으며, 더 좋지 않은 결과로는 이런 설계 선택은 계산 비용이 너무 많이 들게 된다.

## 3.4. Adaptation to Downstream Understanding and Generation Tasks

pretraining 의 두 단계 후, CodeT5+ 는 다양한 모드에 유연하게 작동하여 Seq2Seq generation task, decoder-only tasks 및 understanding-based tasks 를 포함한 다양한 task 를 지원할 수 있다.

### Seq2Seq Generation Tasks

encoder-decoder model 인 CodeT5+ 는 code generation 및 summarization 같은 Seq2Seq generation task 에 자연스럽게 적응할 수 있다.

또한, encoder 를 사용하여 code snippets 를 검색하고, 이를 code generation 을 위해 encoder 및 decoder 모두 사용하는 retrieval-agumented generation model 로 적용할 수 있다.

### Decoder-only Tasks

이 설정에선, encoder input 에 항상 [CLM] 토큰을 주입하고, source sequence 를 prefix context 로 decoder 에 전달한다.

encoder 와 decoder 의 cross-attention layers 의 weight 를 고정한다.

이 전략은 decoder 파트만 활성화하며 기술적으로 전체 model parameter 의 약 절반을 줄인다.

저자는 next-line code completion task 를 사용하여 CodeT5+ 의 decoder 전용 generation 능력을 평가한다.

### Understanding Tasks

CodeT5+ 는 understanding tasks 를 두 가지 방식으로 지원할 수 있다

1. encoder 를 사용하여 text/code embedding 을 얻어 이를 detection task 나 retrieval task 를 위해 binary classifier 에 전달할 수 있다.
22. encoder 를 decoder 와 결합하여 text-to-code retrieval task 에 대한 text-code matching score 를 예측할 수 있다.

# 4. Pretraining and Instruction Tuning

## 4.1. Pretraining Dataset



최근 출시된 GitHub Code 데이터셋을 사용하여 CodeSearchNet 의 pretraining dataset 을 확장했다.

9 개의 PLs (Python, Java, Ruby, JavaScript, GO, PHP, C, C++, C#)를 선택하였고, 허용 라이선스를 가진 코드와 50 ~ 2000 tokens 을 가진 파일들만 필터링 하였다.

또한, GitHub repository name 을 확인하여 CodeSearchNet 과 다른 downstream tasks 에서 중복되는 부분을 필터링했다.

중복 데이터는  exact match 를 기준으로 필터링되어 중복되는 부분이 있을 수 있지만, 이 중복은 모델 성능에 큰 영향을 미치지 않을 것으로 예상된다.

저자는 CodeT5 tokenizer 를 사용하여 다국어 데이터셋을 토큰화했으며, 이로 인해 5150억 개의 토큰이 생성되었다. 이는 CodeSearchNet 보다 약 50배 큰 크기이다.

![](https://velog.velcdn.com/images/whdnjsdyd111/post/0ae8a69d-13f6-417f-b2b0-fa999be692cc/image.png)

Table 1 에는 unimodal code 및 bimodal text-code pretrained dataset 의 통계이다.

표에서 보이듯, GitHub 코드로부터 정리된 데이터셋은 CodeSearchNet bimodal data 의 function level 보다 훨씬 큰 데이터 크기를 가지고 있어, 모델이 pretrain 의 첫 단계에서 풍부한 representation 을 학습할 수 있게 해준다.

CodeT5 와 달리 저자는 CodeSearchNet 의 bimodal 데이터만을 CodeT5+ 의 두 번째 단계 pretrain 에 사용한다.

이 단계에서는 주로 text-code 관련 task 인 text-to-code retrieval 및 generation 에 모델을 적응시킨다.

![](https://velog.velcdn.com/images/whdnjsdyd111/post/02324785-4135-4a1f-84aa-b405f15475df/image.png)

## 4.2. Pretraining Setup

저자는 CodeT5+ models 을 두 그룹으로 pretrain 한다.

1. CodeT5+ 220M 및 770M 은 T5 의 아키텍처에 따라 처음부터 train 된다.
2. CodeT5+ 2B, 6B, 16B 는 decoder 를 CodeGen-mono 2B, 6B, 16B model 에서 초기화하고 encoder 를 CodeGen-mono 350M 에서 초기화한다.

scaing 전략을 주목하자, 

원래의 CodeGen 모델과 비교하여 후자의 CodeT5+ 모델 그룹은 무시할 만한 학습 가능한 매개변수를 도입합니다(2B, 6B, 16B 모델에 대해 350M 인코더와 36M, 67M, 151M의 크로스 어텐션 레이어 하나). 이 두 그룹의 모델에는 각각 CodeT5 토크나이저와 CodeGen 토크나이저를 사용합니다. 사전학습에서는 16개의 A100-40G GPU가 장착된 Google Cloud Platform의 클러스터에서 CodeT5+를 대규모의 단모달 데이터셋과 이후 작은 이중모달 데이터셋에 대해 단계적인 전략으로 사전학습합니다.

첫 번째 단계에서는 10,000번의 학습 단계 동안 모델을 스팬 노이즈 제거 작업으로 예열한 후, 두 개의 CLM 작업과 같은 가중치로 합동 훈련을 100,000번의 단계 동안 진행합니다. 스팬 노이즈 제거 작업에 대해 선형 감쇠 학습률(LR) 스케줄러를 사용하며, 최대 학습률은 2e-4이고 배치 크기는 노이즈 제거에는 2048, CLM에는 512로 설정합니다. 입력 및 출력 데이터를 준비하기 위해 스팬 노이즈 제거 작업의 최대 길이를 512로 설정하고, 코드 완성 CLM의 소스 및 타겟 시퀀스의 최대 길이를 각각 768과 600으로, 디코더만을 사용한 생성 CLM의 최대 길이를 1과 1024로 설정합니다.

두 번째 단계에서는 대조 학습, 매칭 및 두 개의 CLM 손실을 동일한 가중치로 10 에폭 동안 합동으로 최적화합니다. 배치 크기는 256이고 최대 시퀀스 길이는 코드와 텍스트 시퀀스 각각 420과 128로 설정합니다.

모든 실험에서는 0.1 가중치 감쇠를 가진 AdamW 옵티마이저 [Loshchilov and Hutter, 2019]를 사용합니다. 또한 DeepSpeed의 ZeRO Stage 2 [Rasley et al., 2020]와 FP16의 혼합 정밀도 훈련을 사용하여 훈련 가속화를 수행합니다. CodeT5+ 2B, 6B 및 16B의 훈련에는 FP16로 고정된 디코더 가중치를 사용하고 다른 학습 가능한 가중치는 FP32로 유지합니다. CodeT5+ 6B 및 16B 모델에 대해서는 DeepSpeed ZeRO Stage 3의 매개변수 분할을 사용합니다.