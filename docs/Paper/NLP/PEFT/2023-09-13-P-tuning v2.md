---
slug: P-tuning v2
title: "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"
tags: [PEFT, p-tuning, p-tuning v2, prompt tuning]
---

논문 및 이미지 출처 : <https://arxiv.org/pdf/2110.07602.pdf>

# Abstract

**_Prompt tuning_** 은 language model (LM) 을 freezing 한 상태에서 continuous prompt 만 조정하여, training 중 task 당 저장 및 메모리 사용량을 크게 줄이는 것

하지만

- Natural Language Understanding (NLU) 엔, 이전 연구에서 prompt tuning 이 일반적인 크기의 pre-trained model 에 대해서는 잘 수행되지 않음
- 기존의 prompt tuning 방법이 hard sequence labeling task 를 처리할 수 없다는 것 발견
- 위 사항은 범용성의 부족

저자는 적절하게 최적화된 prompt tuning 이 모델 규모와 NLU 작업의 넓은 범위에 걸쳐 보편적으로 효과적일 수 있는 새로운 경험적인 결과를 제시

- fine-tuning 의 성능을 맞추면서도 tuned parameter 가 0.1 ~ 3% 에 불과
- P-tuning v2 는 NLU 용으로 optimized 및 adapted Deep Prompt Tuning (Prefix-tuning, soft prompts) 의 구현
- P-tuning v2 의 보편성과 간단함을 고려하면, fine-tuning 의 대체제로서 향후 연구를 위한 강력한 baseline 으로 기능 가능

# 1. Introduction

Pre-trained LM 은 넓은 범위의 NLU tasks 의 성능을 향상시킨다.

널리 사용되는 방법으로는 다음이 있다.

- **Fine-tuning (FT)** : target task 에 대해, model parameter 전체를 update
  - 좋은 성능을 얻는 반면, all parameters 에 대한 gradient 및 optimizer states 를 저장해야 하므로 training 중 메모리 소비가 심하다.
  - inference 중 각 task 에 대한 model parameter 의 copy 를 유지하는 것은, 보통 LM 이 크기 때문에 여간 불편하다
- **Prompting** : pre-trained LM 의 all parameter 를 freezing 하고 natural language prompt 를 사용하여 LM 에 query 하는 방식
  - sentiment analysis 의 경우, sample (e.g. "Amazing movie!") 에 "[MASK]" 라는 prompt 를 연결하고 pre-trained LM 에게 masked token 이 "good" 과 "bad" 인 확률을 예측하도록 요청하여 label 결정
  - prompting 은 training 을 필요로 하지 않으며 model parameter 의 single copy 만 저장
  - discrete prompting 은 fine-tuning 과 비교하여 성능이 부적절한 경우가 많음
- **Prompt tuning (PT)** : discrete prompts 만 tuning 하는 아이디어
  - continuous embedding (prompts) 을 input word embeddings 의 original sequence 에 추가
  - training 중 continuous prompts 만 updates
  - PT 은 많은 task 에서 prompting 을 개선하지만, 모델 크기가 10B parameter 이하의 경우 FT 를 능가하지 못함
  - hard sequence labeling task 에서 PT 가 prompting 에 비해 성능이 나쁜 것을 관찰


본 논문의 기여는 적절히 최적화된 prompt tuning 이 다양한 모델 규모와 NLU task 범위에서 보편적으로 fine-tuning 과 comparable 하다는 발견

- 이전 연구와 대조적으로 저자는 NLU 에 대한 prompt tuning 의 보편성과 잠재력 발견
- 새로운 개념은 아니며, 생성 및 지식 탐색을 위해 설계된 Deep Prompt Tuning (Prefix-tuning, soft prompts) 의 최적화 및 적응된 구현
- input layer 뿐 아니라 pre-trained LM 의 모든 layer 에 continuous prompt 적용
- Deep Prompt Tuning 은 continuous 의 용량을 증가시키고 모델 크기와 hard task 의 간격을 좁히는 데 기여
- fine-tuning 과 유사한 성능 보장을 위한 최적화 및 구현의 중요한 세부 사항 제시


실허 결과 P-tuning v2 는 300M 에서 10B parameter 까지 다양한 모델 규모 및 extractive question answering 및 named entity recognition 같은 hard sequence tagging task 을 포함하여 다양한 task 에서 fine-tuning 과 유사한 성능을 보이며, task 당 trainable parameter 의 비율이 0.1% ~ 3% fine-tuning 과 비교하여 training 시간 메모리 비용 및 task 당 저장 비용을 크게 줄임

# 2. Preliminaries

# 3. P-Tuning v2

# 4. Experiments

# 5. Conclusions