---
slug: IA³
title: "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"
tags: [PEFT, ICL, few-shot in-context learning, IA³, T-Few]
---

논문 및 이미지 출처 : <https://arxiv.org/pdf/2205.05638.pdf>

# Abstract

Few-shot in-context learning (ICL)은 pre-trained language model (LM) 이 input 의 일부에 적은 수의 training examples 를 feeding 하여 unseen task 를 gradient-based training 없이 수행하게 했다.

- ICL 은 all training examples 를 처리해야 하여 계산, 메모리 및 저장 비용이 큼

Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update, etc) 은 모델이 new task 를 수행하도록 훈련된 parameter small set 을 제공하는 paradigm

---

본 논문은 few-shot ICL 과 PEFT 를 비교하여 PEFT 가 better accuracy 및 lower computational cost 제공을 입증한다.

이 과정에 learned vector 로 activations 를 확장하는 new PEFT 인 $(IA)^3$ 소개

- tiny new parameter 만 도입하여 더 강력한 성능
- T0 model 에 기반하는 _F-Few_ 로, new tasks 에 대한 task-specific tuning 또는 modifications 없이 적용할 수 있는 simple recipe 제안
- T-Few 를 RAFT benchmark 에 적용함으로써 unseen tasks 에 대한 효과성 검증
- super-human 성능을 최초로 6% 로 능가하여 SOTA 달성

# 1. Introduction

pre-trained LM 은 NLP 의 중요한 요소

- 모델로 target task 에 대한 _data-efficient_ 를 크게 향상. 즉, pre-trained LM 을 초기화에 사용하여 less labeled data 에 better results
- common approach 는 pre-trained LM 의 parameter 를 초기화에 사용한 다음, target downstream task 에 gradient-based fine-tuning 수행
  - fine-tuning 은 SOTA 를 도출하지만, new parameter set 값을 가지는 single task 에 특화된 모델을 생성하여 여러 downstream task 에 fine-tuning 할 경우 불필요하게 복잡해짐

[Language models are unsupervised multitask learners, Language models are
few-shot learners] 에서 인기 있는 alternative approach 는 _in-context learning_ (ICL) 이다.

- ICL 은 모델이 _prompted_ example 을 inputting 하여 downstream task 를 수행하도록 유도
- Few-shot prompting 은 input-target pairs 를 예측이 필요한 single unlabeled example 을 따라, human-understandable instructions 및 example 로 변환
- ICL 은 gradient-based training 이 필요하지 않아, single model 은 즉시 다양한 task 에 수행 가능
- 따라서, ICL 은 pre-training 중에 모델이 학습한 능력에만 의존하며, 이 특성으로 인해 ICL 방법에 관심이 쏠림

ICL 은 위 이점에도 불구하고 여러 단점 존재

- 모델이 예측할 때마다 prompted input-target pairs 를 모두 처리하면 상당한 계산 비용 발생
- ICL 은 일반적으로 fine-tuning 보다 성능 떨어짐
- prompt 의 exact formatting (단어 선택 및 예제 순서 포함)이 모델의 성능에 미치는 영향을 예측 불가능

최근 ICL 이 incorrect labels 를 제공해도 잘 수행하는 점을 보여주며, 얼마나 많은 학습이 이루어지는지에 대한 의문 제기

최근의 연구는 ICL이 부정확한 레이블을 제공받아도 잘 수행할 수 있다는 점을 보여주는데, 이는 학습이 얼마나 많이 이루어지는지에 대한 의문을 불러일으킴

---

model 이 new task 수행을 위해 minimal updates 로 모델을 활성화하는 추가적인 패러다임인 _parameter-efficient fine-tuning_ (PEFT)

- pre-trained model 에 추가 또는 선택된 small parameter 만 update 하여 fine-tuning
- 최근 전체 모델의 subset parameter (e.g. 0.01%)만 update 하거나 추가하여 fine-tuning 과 동등한 성능 달성
- 특정 PEFT 은 batch 내의 examples 를 다르게 처리하는 _mixed-task batches_ 를 허용하여 PEFT 및 ICL 은 multitask models 모두에 적합

PEFT 의 이점은 fine-tuning 의 일부 단점을 해소하지만, 매우 적은 양의 label data 만 사용 가능할 경우 PEFT 방법이 잘 작동하는지에 대한 연구가 적다.

본 논문의 주요 목표는 이 공백을 메우기 위해 model, PEFT method 및 fixed hyperparameter set 을 사용하여, 모델의 일부 parameter 만 업데이트하며 novel, unseen task 에서 강력한 성능을 달성하는 recipe 제안

- 저자의 approach 는 T0 model 을 기반
  - 이 모델은 T5 의 변형으로 다양한 prompt dataset 의 multitask mixture 을 fine-tuning 한 것
- classification 및 multiple-choice tasks 성능 향상을 위해 unlikelihood 및 length normalization-based loss term 추가
- intermediate activations 를 learned vectors 와 곱하는 PEFT method $(IA)^3$ 개발
  - $(IA)^3$ 은 parameter 를 최대 10,000배 적게 업데이트하며 full fine-tuning 보다 강력한 성능 달성
- 저자의 _T-Few_ recipe 는 ICL (16배 큰 모델과 비교) 에 비해 상당히 더 나은 성능 발휘
  - real-world few-shot learning 벤치마크 RAFT 에서 처음으로 human 능가
  - less compute 및 inference 중 mixed-task batches 가능

# 2. Background

# 3. Designing the T-Few Recipe

PEFT 는 small storage requirement 및 computational cost 로 model 을 new task 에 adapting 할 가능성을 제시하여 ICL 의 대안으로 유망함

- 따라서 저자는 computational 및 storage cost 를 최소화하며 inference 중 mixed-task batches 가 가능한, limited labeled examples 로 new task 에 높은 정확도를 달성할 수 있는 recipe 개발이 목표

여기서 _recipe_ 란 new task 에서 강력한 성능을 제공하는 model 및 hyperparameter 설정을 의미하며, manual tuning 이나 per-task adjustments 없이 강력한 성능을 보장할 수 있는 것이다.

이를 평가하기 위해 limited labeled data 만 사용 가능한 few-shot settings 에서 approach 가 실질적인 옵션임을 보장할 수 있다.

## 3.1 Model and Datasets

limited labeled examples 로 fine-tuning 후에도 성능이 높은 이상적인 pre-trained model 선택에 있어, PEFT method 적용으로 best 성능을 달성한 T0 채택.

- T0 는 T5 기반이며, unlabeled text data 인 large corpus 의 masked language modeling objective 로 pre-training 한 encoder-decoder Transformer
- dataset multitask mixture 을 기반으로 T5 를 fine-tuning 하여 zero-shot generalization 가능 (i.e. additional gradient-based training 없이 task 수행)
- T0 training 에 사용된 dataset examples 는 Public Pool of Prompts (P3) 의 prompt templates 를 적용한 prompted 이며, prompted text-to-text format 으로 변환
- T0 는 3B 와 11B parameter 를 제공하며 각각 "T0-3B" 및 "T0" 로 지칭

---

T0 는 zero-shot generalization 을 위해 설계되었지만, 저자는 few labeled example 만으로 fine-tuning 후에도 강력한 성능을 달성하는 것을 증명

- T0 generalization 능력 테스트를 위해 multitask training mixture 에서 제외할 task 선택
  - sentence completion (COPA, H-SWAG 및 Stroy Cloze datasets)
  - natural language inference (ANLI, CB 및 RTE)
  - coreference resolution (WSC 및 Winogrande)
  - word sense disambiguation (WiC)



T0의 일반화 능력을 테스트하기 위해 Sanh et al. [1]은 멀티태스크 훈련 믹스처에서 제외할 일련의 작업(및 해당 데이터셋)을 선택했습니다. 구체적으로 문장 완성 (COPA [37], H-SWAG [38], Story Cloze [39] 데이터셋), 자연어 추론 (ANLI [40], CB [41], RTE [42]), 공통 참조 해결 (WSC [43] 및 Winogrande [44]), 그리고 단어 의미 재구분 (WiC [45])을 선택했습니다. 일반화 능력의 평가는 이러한 제외된 데이터셋에서의 성능 측정을 통해 직접 수행할 수 있습니다.

또한 RAFT 벤치마크 [2]에서 T-Few의 능력을 나중에 섹션 4.3에서 테스트할 예정입니다. RAFT 벤치마크는 확인 세트 없이 보유된 테스트 세트를 포함한 "실제 세계" 적은 데이터 작업의 컬렉션입니다. ANLI, WiC, WSC는 크리에이티브 커먼즈 라이선스에 따라 라이선스가 부여되었습니다. Winogrande는 아파치 라이선스에 따라 라이선스가 부여되었습니다. COPA는 BSD-2 절 조건 라이선스에 따라 라이선스가 부여되었습니다. RTE와 CB의 라이선스를 찾을 수 없었지만, 이들은 SuperGLUE의 일부로서 연구 환경에서 사용이 허용된 데이터셋이라고 언급되어 있습니다.

평가에는 "순위 분류"를 사용하며, 모든 가능한 레이블 문자열에 대한 모델의 로그 확률을 순위 지정하고, 가장 높은 순위를 가진 선택이 올바른 답인 경우 모델의 예측을 올바르다고 간주합니다. 순위 분류 평가는 분류 및 다중 선택 작업 모두와 호환됩니다. 모델 성능은 사용된 프롬프트 템플릿에 따라 크게 다를 수 있기 때문에 각 데이터셋에 대한 모든 프롬프트 템플릿에서 중앙 정확도와 데이터 하위 집합 간의 중앙값을 보고합니다. 