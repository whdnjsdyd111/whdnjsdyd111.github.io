---
slug: IA³
title: "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"
tags: [PEFT, ICL, few-shot in-context learning, IA³, T-Few]
---

논문 및 이미지 출처 : <https://arxiv.org/pdf/2205.05638.pdf>

# Abstract

Few-shot in-context learning (ICL)은 pre-trained language model (LM) 이 input 의 일부에 적은 수의 training examples 를 feeding 하여 unseen task 를 gradient-based training 없이 수행하게 했다.

- ICL 은 all training examples 를 처리해야 하여 계산, 메모리 및 저장 비용이 큼

Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update, etc) 은 모델이 new task 를 수행하도록 훈련된 parameter small set 을 제공하는 paradigm

---

본 논문은 few-shot ICL 과 PEFT 를 비교하여 PEFT 가 better accuracy 및 lower computational cost 제공을 입증한다.

이 과정에 learned vector 로 activations 를 확장하는 new PEFT 인 $(IA)^3$ 소개

- tiny new parameter 만 도입하여 더 강력한 성능
- T0 model 에 기반하는 _F-Few_ 로, new tasks 에 대한 task-specific tuning 또는 modifications 없이 적용할 수 있는 simple recipe 제안
- T-Few 를 RAFT benchmark 에 적용함으로써 unseen tasks 에 대한 효과성 검증
- super-human 성능을 최초로 6% 로 능가하여 SOTA 달성

# 1. Introduction

pre-trained LM 은 NLP 의 중요한 요소

- 모델로 target task 에 대한 _data-efficient_ 를 크게 향상. 즉, pre-trained LM 을 초기화에 사용하여 less labeled data 에 better results
- common approach 는 pre-trained LM 의 parameter 를 초기화에 사용한 다음, target downstream task 에 gradient-based fine-tuning 수행
  - fine-tuning 은 SOTA 를 도출하지만, new parameter set 값을 가지는 single task 에 특화된 모델을 생성하여 여러 downstream task 에 fine-tuning 할 경우 불필요하게 복잡해짐

[Language models are unsupervised multitask learners, Language models are
few-shot learners] 에서 인기 있는 alternative approach 는 _in-context learning_ (ICL) 이다.

- ICL 은 모델이 _prompted_ example 을 inputting 하여 downstream task 를 수행하도록 유도
- Few-shot prompting 은 input-target pairs 를 예측이 필요한 single unlabeled example 을 따라, human-understandable instructions 및 example 로 변환
- ICL 은 gradient-based training 이 필요하지 않아, single model 은 즉시 다양한 task 에 수행 가능
- 따라서, ICL 은 pre-training 중에 모델이 학습한 능력에만 의존하며, 이 특성으로 인해 ICL 방법에 관심이 쏠림

ICL 은 위 이점에도 불구하고 여러 단점 존재

- 모델이 예측할 때마다 prompted input-target pairs 를 모두 처리하면 상당한 계산 비용 발생
- ICL 은 일반적으로 fine-tuning 보다 성능 떨어짐
- prompt 의 exact formatting (단어 선택 및 예제 순서 포함)이 모델의 성능에 미치는 영향을 예측 불가능

최근 ICL 이 incorrect labels 를 제공해도 잘 수행하는 점을 보여주며, 얼마나 많은 학습이 이루어지는지에 대한 의문 제기

최근의 연구는 ICL이 부정확한 레이블을 제공받아도 잘 수행할 수 있다는 점을 보여주는데, 이는 학습이 얼마나 많이 이루어지는지에 대한 의문을 불러일으킴

---

model 이 new task 수행을 위해 minimal updates 로 모델을 활성화하는 추가적인 패러다임인 _parameter-efficient fine-tuning_ (PEFT)

- pre-trained model 에 추가 또는 선택된 small parameter 만 update 하여 fine-tuning
- 최근 전체 모델의 subset parameter (e.g. 0.01%)만 update 하거나 추가하여 fine-tuning 과 동등한 성능 달성
- 특정 PEFT 은 batch 내의 examples 를 다르게 처리하는 _mixed-task batches_ 를 허용하여 PEFT 및 ICL 은 multitask models 모두에 적합

PEFT 의 이점은 fine-tuning 의 일부 단점을 해소하지만, 매우 적은 양의 label data 만 사용 가능할 경우 PEFT 방법이 잘 작동하는지에 대한 연구가 적다.

본 논문의 주요 목표는 이 공백을 메우기 위해 model, PEFT method 및 fixed hyperparameter set 을 사용하여, 모델의 일부 parameter 만 업데이트하며 novel, unseen task 에서 강력한 성능을 달성하는 recipe 제안

- 저자의 approach 는 T0 model 을 기반
  - 이 모델은 T5 의 변형으로 다양한 prompt dataset 의 multitask mixture 을 fine-tuning 한 것
- classification 및 multiple-choice tasks 성능 향상을 위해 unlikelihood 및 length normalization-based loss term 추가
- intermediate activations 를 learned vectors 와 곱하는 PEFT method $(IA)^3$ 개발
  - $(IA)^3$ 은 parameter 를 최대 10,000배 적게 업데이트하며 full fine-tuning 보다 강력한 성능 달성
- 저자의 _T-Few_ recipe 는 ICL (16배 큰 모델과 비교) 에 비해 상당히 더 나은 성능 발휘
  - real-world few-shot learning 벤치마크 RAFT 에서 처음으로 human 능가
  - less compute 및 inference 중 mixed-task batches 가능

# 2. Background

# 3. Designing the T-Few Recipe

PEFT 는 small storage requirement 및 computational cost 로 model 을 new task 에 adapting 할 가능성을 제시하여 ICL 의 대안으로 유망함

- 따라서 저자는 computational 및 storage cost 를 최소화하며 inference 중 mixed-task batches 가 가능한, limited labeled examples 로 new task 에 높은 정확도를 달성할 수 있는 recipe 개발이 목표

여기서 _recipe_ 란 new task 에서 강력한 성능을 제공하는 model 및 hyperparameter 설정을 의미하며, manual tuning 이나 per-task adjustments 없이 강력한 성능을 보장할 수 있는 것이다.

이를 평가하기 위해 limited labeled data 만 사용 가능한 few-shot settings 에서 approach 가 실질적인 옵션임을 보장할 수 있다.

## 3.1 Model and Datasets

limited labeled examples 로 fine-tuning 후에도 성능이 높은 이상적인 pre-trained model 선택에 있어, PEFT method 적용으로 best 성능을 달성한 T0 채택.

- T0 는 T5 기반이며, unlabeled text data 인 large corpus 의 masked language modeling objective 로 pre-training 한 encoder-decoder Transformer
- dataset multitask mixture 을 기반으로 T5 를 fine-tuning 하여 zero-shot generalization 가능 (i.e. additional gradient-based training 없이 task 수행)
- T0 training 에 사용된 dataset examples 는 Public Pool of Prompts (P3) 의 prompt templates 를 적용한 prompted 이며, prompted text-to-text format 으로 변환
- T0 는 3B 와 11B parameter 를 제공하며 각각 "T0-3B" 및 "T0" 로 지칭

---

T0 는 zero-shot generalization 을 위해 설계되었지만, 저자는 few labeled example 만으로 fine-tuning 후에도 강력한 성능을 달성하는 것을 증명

- T0 generalization 능력 테스트를 위해 multitask training mixture 에서 제외할 task 선택
  - sentence completion (COPA, H-SWAG 및 Stroy Cloze datasets)
  - natural language inference (ANLI, CB 및 RTE)
  - coreference resolution (WSC 및 Winogrande)
  - word sense disambiguation (WiC)


또한 RAFT 벤치마크에서 T-Few 의 능력을 Sectiopn [4.3](#43-performance-on-real-world-few-shot-tasks-raft) 에서 테스트

- RAFT 는 validation 및 held-out test set 없이 unseen "real-world" few-shot collection

평가에는 "rank classification" 을 사용하며, all possible label strings 에 대한 모델의 log-probabilities 는 순위를 지정하며, 가장 높은 순위의 choice 는 가 올바른 답일 경우 모델 예측이 올바르다 간주

Rank classification evaluation 은 classification 및 multiple choice tasks 모두와 호환

모델 성능은 prompt template 에 따라 크게 다르므로 각 데이터셋에 대한 all prompt template 를 median accuray 로 보고

test label 이 public 이 아닐 경우 (e.g. SuperGLUE) test set 또는 validation set 에 대한 정확도를 보고

본문에선 위에서 언급한 9개 데이터셋에 대한 median accuracy 보고

## 3.2 Unlikelihood Training and Length Normalization

PEFT 연구 전에, 저자는 LM 의 few-shot fine-tuning 의 성능 향상을 위한 두 가지 additional loss terms 탐구

LM 은 보통 cross-entropy loss $L_{LM} = -\frac{1}{T} \sum_t \log p (y_t | \text{x}, y_{<t})$ 로 훈련된다.

- input sequence $\text{x}$ 에 대한 correct target sequence $\text{y} = (y_1, y_2, \dots, y_T)$ 의 probability 가 증가하도록 훈련



## 3.3 Parameter-efficient fine-tuning with $(IA)^3$

## 3.4 Pre-training $(IA)^3$

## 3.5 Combining the ingredients

# 4. Outperforming ICL with T-Few

## 4.1 Performance on T0 tasks

## 4.2 Comparing computational costs

### Inference cost

### Training cost

### Storage cost

### Memory usage

## 4.3 Performance on Real-world Few-shot Tasks (RAFT)

## 4.4 Ablation experiments

# 5. Conclusion