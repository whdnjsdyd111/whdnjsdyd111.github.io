---
slug: SMP
title: "Pruning Pre-trained Language Models Without Fine-Tuning"
tags: [PEFT, pruning, no fine-tuning]
---

논문 및 이미지 출처 : <https://aclanthology.org/2023.acl-long.35.pdf>

# Abstract

Pre-trained Language Models (PLMs) 의 overparameterize 문제를 극복하기 위해, pruning 이 unimportant weights 를 직접적으로 제거하여 간단하고 직관적인 압축 방법으로 널리 사용한다.

이전의 first-order method 는 PLM 을 extremely high sparsity 로 압축하는데 성공하여, 약간의 성능 하락만 발생한다.

이런 방법들은 movement pruning 같은 first-order information 을 사용하여 PLM 을 pruning 하고 remaining weights 를 fine-tuning 하는 방식으로 작동한다.

본 논문에서는 first-order pruning 에 대한 fine-tuning 이 불필요하다 주장한다.

- first-order pruning 만으로도 PLM fine-tuning 없이 downstream task 에 수렴할 수 있기 때문
- 위 동기로 저자는 **Static Model Pruning (SMP)** 제안
  - target sparsity level 에 달성하는 동안 PLM 을 downstream task 에 adapting 하기 위해 first-order pruning 사용
  - 추가로, SMP 를 더 개선하기 위해 new masking function 와 training objective 설계
- 다양한 sparsity levels 에서 광범위한 실험 결과, SMP 는 first-order 및 zero-order 보다 상당한 개선
- 이전의 first-order 과 달리 SMP 는 low sparsity 에서도 적용 가능하며 zero-oder 보다 우수한 성능
- SMP 는 fine-tuning 이 필요하지 않아 다른 방법보다 parameter-efficient

# 1. Introduction

BERT 같은 PLMs 는 large-scale corpus 로부터 knowledge transfering 으로 downstream task 에 강력한 성능을 보여주지만, large-scale parameter 가 필요하다.

이러한 parameter 는 대부분의 downstream task 에는 너무 많아서, transfer 및 copy 에 부담이 된다.

PLM 을 압축하는 pruning 이 널리 사용되며, unimportant weights 를 제거하고 이를 zero 로 설정함으로써 이루어진다.

기존의 pruning 은 original complete network 대신 sparse subnetwork 를 사용함으로써 대부분의 weight 를 제거하여 original accuracy 를 유지할 수 있다.

- Magnitude Pruning (Han et al, 2015) : zero-order information 을 사용하여 weight 의 absolute value 게 기반하여 pruning 결정
  - downstream task adapting 과정에 PLM 의 weight value 는 이미 original value 로 결정됨
- 위 단점을 극복하기 위해 movement pruning (Sanh et al, 2020) 은 weight 가 absolute value 가 아닌 training 중 어떻게 변하는지에 따라 선택되는 first-order information 사용
  - 위와 같이 대부분 movement pruning 과 같이 fine-tuning 과 함께 수행하여 training 중 sparsity 를 점진적으로 증가시킨다.
- Lottery Ticket Hypothesis (LTH) (Frankle and Carbin, 2018) 의 발전에 따라, 일부는 pruning 을 통해 PLM 의  특정 subnetwork 를 찾은 다음 pre-trained weight 에서 이 subnetwork 를 fine-tuning
  - 또한 fine-tuned subnetwork 가 전체 PLM 의 성능과 일치하는 경우, 이 subnetwork 를 winning ticket 이라 함

본 논문에서는 간단하지만 효율적인 first-order method 제안

- 이전의 pruning 과 달리 fine-tuning 없이 pruning 만을 통해 PLM adapting
- 이 방법은 movement pruning 에 기반하여 pruning 결정을 내리며, movement pruning 에서의 movement 와는 다르다.
- 저자의 방법은 성능 향상을 위해 remaining weight 를 PLM 에 잘 맞게 하기 위한 **new masking function** 을 제안
- 또한 task-specifc head 에서 weight 를 fine-tuning 하지 않고 저자의 head initialization method 를 사용하여 더욱 효율적
- PLM 고정함으로써, 다른 first-order 과 비교하여 trainable parameter 반을 줄이며, 다양한 sparisty levels 에서 각 downstream task 에 대한 new parameter 로 binary mask 만 도입
- 다양한 sparsity level 에서 광범위한 실험으로 SOTA pruning 을 능가
- 이전의 first-order (Sanh et al, 2020)이 low sparsity 에서 성능이 좋지 않은 반면, 저자는 low sparsity 에서도 적용 가능하며 zero-order 보다 우수한 성능 달성

# 2. Related Work

PLM 의 overfitting 해결을 위한 여러 압축 방법이 제안되었다. 예로, model pruning (Han et al., 2015; Molchanov et al., 2017; Xia et al., 2022), knowledge distillation (Jiao et al., 2020; Wang et al., 2020), 양자화(Shen et al., 2020; Qin et al., 2022), quantization (Shen et al., 2020; Qin et al., 2022) 및 matrix decomposition (Lan et al., 2020) 등이 있다.

- Pruning 은 모델에 unimportant weights 를 식별하고 제거하는 데 초점을 둔다.
  - Zero-order 과 first-order 는 PLM pruning 에 널리 사용된다.
  - Zero-order : magnitude pruning (Han et al, 2015)는 단순히 weight 의 absolute value 를 기반으로 pruning 수행
  - Fist-order : pruning 결정을 위해 first-order Taylor expansion 을 기반으로 하며, $L_0$ regularization (Louizos et al, 2017) 은 $L_0$ norm regularization 을 추가하여 hard-concrete distribution 으로 sampling 하여 remaining weights 를 줄인다.
  - Movement pruning (Sanh et al, 2020) : _staight-through estimator_ (Bengio et al, 2013)을 사용하여 first-order information 계산
- pruning 을 기반으로, Frankle and Carbin (2018) 은 Lottery Ticket Hypothesis (LTH) 제안
  - 개별로 훈련시킬 때 거의 동일한 성능을 달성하는 subnetwork (winning ticket) 존재 확인
  - LTH 발전에 따라, PLM 에 중점을 둔 연구 등장
  - BERT 가 40 ~ 90% 의 sparsity 를 가진 winning tickets 을 포함하며, NLP 에서 winning ticket 이 downstream task 에 transfer 가능함을 발견
  - Liang et al. (2021) 은 winning tickets 의 일반화 성능이 임계점 이후 개선되고 나빠지는 현상을 이용하여, LTH 가 downstream task 성능에 성공적인 향상을 주는 것을 보여줌

# 3. Background

$\text{a} = \text{Wx}$ 를  PLM 의 fully-connected layer 라 하자.

- $\text{W} \in \mathbb{R}^{n\times n}$ : weight matrix
- $\text{x} \in \mathbb{R}^n$ 및 $\text{a} \in \mathbb{R}^n$ : input 및 output

pruning 은 $\text{a} = (\text{W} \odot \text{M})\text{x}$ 으로 표현

- $\text{M} \in \{0,1\}^{n\times n}$ : binary mask

PLMs 에서 두 가지 common pruning 을 리뷰 해본다.

- magnitude pruning (Han et al, 2015)
  - absolute value $\text{M} = \text{Top}_v(\text{S})$ 에 따라 top $v$ weight 를 유지하여 결정하는 zeroth-order information 에 의존
  - importance score $\text{S} \in \mathbb{R}^{n\times n}$ 은 다음과 같다.
  - $$
    \begin{equation}
        \begin{align*}
            S^{(T)}_{i,j} &= \left | W^{(T)}_{i,j} \right | \\
            &= \left | W_{i,j} - \alpha_w \sum_{t<T} \left ( \frac{\partial \mathcal{L} }{\partial W_{i,j}} \right )^{(t)} \right  |
        \end{align*}
    \end{equation}
    $$
    - $S^{T}_{i,j}$ : $T$ steps update 이후 $W^{(T)}_{i,j}$ 에 대응하는 importance score
    - $\mathcal{L} 및 \alpha_w$ : $W_{i,j}$ 의 learning object 및 learning rate
  - fine-tuning 중 high absolute values 의 weight 선택
- movement pruning (Sanh et al, 2020)
  - gradient 를 사용하여 importance scores $\text{S}$ 를 학습하는 것에 의존
  - $\text{S}$ 의 gradient 는 _staight-through estimator_ 를 사용하여 근사하며, 이는 $\text{M}$ 에서 직접적으로 gradient 사용
  - importance scores $\text{S}$ 는 다음과 같다.
  - $$
    \begin{equation}
      S^{(T)}_{i,j} = -\alpha_s \sum_{t<T} \left (\frac{\partial \mathcal{L}}{\partial W_{i,j}} \right )^{(t)} W_{i,j}^{(t)}
    \end{equation}
    $$
    - $\alpha_s$ : $\text{S}$ 의 learning rate
  - magnitude pruning 과 비교하여, movement pruning 은 absolute value 를 증가시키는 weight 를 선택
- target sparsity 달성을 위한 한 가지 공통된 방법은 _automated gradual pruning_ (Michael H. Zhu, 218)
  - sparsity level $v$ 는 training step $t_0: v^t = v_f + (v_0 - v_f) (1-\frac{t-t_0}{N \triangle t})^3$ 에서 시작하는 cubic sparsity scheduler 를 사용하여 점진적으로 증가된다.
    - $v_0$ 및 $v_f$ : initial 및 target sparsity
    - $N$ : overall pruning steps
    - $\triangle t$ : pruning frequency

training 중, 이런 방법들은 pruning 과 fine-tuning 을 동시에 수행하기 위해 $\text{W}$ 와 $\text{S}$ 모두 업데이트해야 한다.

fine-tuned weight 는 pre-trained value 에 가깝게 유지되므로 (Sanh et al, 2020), magnitude pruning 의 importance scores 는 pre-trained value 에 영향을 받아 high sparsity 에서의 성능을 제한한다.

하지만 magnitude pruning 은 여전히 low sparsity 에서 movement pruning 보다 우수한 성능을 보인다.

# 4. Static Model Pruning

본 논문에선 간단한 first-order pruning 인 **Static Model Pruning (SMP)** 제안

- 이는 PLM 의 pruning 을 더 efficient 및 transferable 하기 위해 $\text{W}$ 를 고정

저자의 importance scores $\text{S}$ 는 다음과 같다.

$$
\begin{equation}
  S^{(T)}_{i,j} = -\alpha_s W_{i,j} \sum_{t<T} \left (\frac{\partial \mathcal{L}}{\partial W_{i,j}'} \right )^{(t)}
\end{equation}
$$

- $W'_{i,j}$ : $W_{i,j}M_{i,j}$
- $W_{i,j}$ 는 freezing 하기 때문에 binary masking term $M_{i,j}$ 은 유지
- $S_{i,j}$ : $W_{i,j} \frac{\partial \mathcal{L}}{\partial W'_{i,j}} < 0$ 일 때 증가
- remaining weight $W'_{i,j} = W_{i,j}$ 의 경우, movement trending $-\frac{\partial \mathcal{L}}{\partial W'_{i,j}}$ 가 $W_{i,j}$ 의 absolute value 를 증가시키는 것을 의미
- removed weight $W'_{i,j} = 0$ 의 경우, movement trending 이 0 을 $W_{i,j}$ 에 가깝도록 하는 것을 의미

## 4.1 Masking Function

