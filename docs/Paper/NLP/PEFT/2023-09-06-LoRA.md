---
slug: LoRA
title: "LoRA: Low-Rank Adaptation of Large Language Models"
tags: [PEFT, Adapter]
---

논문 및 이미지 출처 : <https://arxiv.org/pdf/2106.09685.pdf>

# Abstract

GPT-3 175B 의 파라미터를 fine-tuning 하기에는 매우 부담.

이에 저자는 **Lo**w-**R**ank **A**daptation, **LoRA** 제안

- pre-trained model weights 를 freeze
- Transformer layer 에 trainable rank decomposition 을 주입
- downstream task 에 대한 trainable parameter 를 매우 크게 줄임

Adam 으로 fine-tuning 한 GPT-3 175B 와 비교하여, LoRA 는 

- trainable parameter 수를 10,000배 줄이고
- GPU 메모리 요구사항 3배 줄임
- RoBERTa, DeBERTa, GPT-2 및 GPT-3 에서도 더 좋은 성능

<hr/>

- LoRA 는 adapter 와 달리, **추가적인 inference latency 없음**
- language model (LM) 의 adaptation 에서 rank-deficiency 에 대한 경험적 연구 제공하여 LoRA 의 효과 제공

# 1. Introduction

보통 multiple downstream 에 적응하기 위해 하나의 LLM 에 의존한다.

task 에 adaptation 할 때 일반적으로 _fine-tuning_ 을 하지만, 모든 파라미터를 훈련하는 것이 큰 단점이다.

이에, 몇몇 파라미터만 adapting 하거나 new task 를 위한 외부 모듈을 학습하는 시도도 있다.

task 마다 pre-trained model 에 적은 수의 task-specific parameter 를 저장하고 로드하여, 모델 사용 시 효율이 향상한다.

하지만, depth 확장이나 sequence 길이를 줄이는 등 모델 품질과 효율성간의 trade-off 설정이 힘들다.

저자는 이전 연구에서 learned over-parameterized models 이 사실은 low intrinsic dimension 에 존재한다는 것을 보고 영감을 받았다.

model adaptation 중 weight 의 변화가 낮은 "intrinsic rank" 를 가질 것이라는 가설을 기반으로 Low-Rank Adaptation (LoRA) approach 를 제안

- pre-trained weights 를 고정한 채, model adaptation 중 dense layer 의 변화에 대한 rank decomposition matrices 를 최적화하여 신경망의 일부 dense layer 를 간접적으로 학습하게 함
- 예로, GPT-3 175B 사용 시, LoRA 를 사용하면 low rank 는 full rank ($d$) 가 12,288 와 같아도 충분
- 이로써 LoRA 는 저장 및 계산 효율적

![Figure 1](image-9.png)

LoRA 핵심 이점

- pre-trained model 은 여러 task 에 대해, 매우 작은 LoRA 모듈을 구축하여 공유 및 사용 가능
  - shared model 을 freeze 하고 Fig 1 의 행렬 $A$ 와 $B$ 를 교체하여 task 를 효율적으로 전환
  - 저장 요구사항 및 task-switching overhead 크게 줄임
- LoRA 는 adaptive optimizer 를 사용할 때 더 효율적인 학습 가능 및 hardware barrier 3배 줄임
  - 대부분의 parameter 에 대한 optimizer states 를 유지하거나 gradient 계산 불필요
  - 주입된 smaller low-rank matrices 만 최적화
- 이 설계는 사용 시, trainable matrices 및 frozen weights 를 병합할 수 있게 함
  - full fine-tuned model 과 비교하여, _inference latency 가 없음_
- 